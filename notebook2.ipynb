{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('shiing': conda)"
  },
  "interpreter": {
   "hash": "414c3711fdd48b2f544ed65b2d7540fc36858fd70ee4f390e2b75c8d3b465c57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook2: Correlation-based Recommender Systems"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load dataset from pro-processed .npy files by using `np.load`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_pair = np.load('../dataset/train_pair.npy')\n",
    "test_pair = np.load('../dataset/test_pair.npy')\n",
    "train_ratings = np.load('../dataset/train_ratings.npy')\n",
    "test_ratings = np.load('../dataset/test_ratings.npy')\n",
    "n_user, n_item = train_pair[:,0].max()+1, train_pair[:,1].max()+1"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## Implement Correlation-based (user-based) recommender systems\n",
    "- Inpout: training set.\n",
    "\n",
    "- Output: return predicted ratings for (user id, item id) user-item pairs in testing set.\n",
    "\n",
    "- Goal: make prediction for testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Algo 1: Correlation-based (user-based) recommender systems (stored sim-matrix)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.96121666  2.89458581 -0.37234958 ... -2.72906623  2.05574082\n  0.70626127]\n"
     ]
    }
   ],
   "source": [
    "pred_algo1 = np.zeros(len(test_ratings))\n",
    "# Step 1: compute mean-centering ratings\n",
    "MC_ratings = train_ratings.copy()\n",
    "for u in range(n_user):\n",
    "    # find all indice for user-u use np.where\n",
    "    index_tmp = np.where(train_pair[:,0] == u)[0]\n",
    "    if len(index_tmp) == 0:\n",
    "        continue\n",
    "    MC_ratings[index_tmp] = MC_ratings[index_tmp] - MC_ratings[index_tmp].mean()\n",
    "print(MC_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2(a): compute pairwise cosine similarity between all users\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim_user(u,v,train_pair,train_ratings):\n",
    "    index_u = np.where(train_pair[:,0] == u)[0]\n",
    "    index_v = np.where(train_pair[:,0] == v)[0]\n",
    "    item_u = train_pair[index_u][:,1]\n",
    "    item_v = train_pair[index_v][:,1]\n",
    "    # find co-rating items by `set`\n",
    "    item_co = list(set(item_u).intersection(set(item_v)))\n",
    "    if len(item_co) <= 0:\n",
    "        # a tuning parameter\n",
    "        return 0.0\n",
    "    else:\n",
    "        # find the co-rating vectors by using `np.isin`\n",
    "        vec_u, vec_v = train_ratings[index_u], train_ratings[index_v]\n",
    "        vec_co_u, vec_co_v = vec_u[np.isin(item_u, item_co)], vec_v[np.isin(item_v, item_co)]\n",
    "        return np.dot(vec_co_u, vec_co_v) / norm(vec_co_u) / norm(vec_co_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "# dense sim_matrix\n",
    "S = np.zeros((n_user,n_user))\n",
    "# sparse matrix \n",
    "# S = lil_matrix((n_user, n_user))\n",
    "for u in range(n_user):\n",
    "    for v in range(u):\n",
    "        # S[u,v] = cossim_user(u,v,train_pair,train_ratings)\n",
    "        # use mean-centering ratings to compute the similarity\n",
    "        S[u,v] = cossim_user(u,v,train_pair,MC_ratings)\n",
    "S = S + S.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2(b): use `dict` to store rated users for all items\n",
    "index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: make prediction if S is stored\n",
    "glb_mean = train_ratings.mean()\n",
    "\n",
    "for j in range(len(test_pair)):\n",
    "    user_tmp, item_tmp = test_pair[j,0], test_pair[j,1]\n",
    "    index_tmp = index_item[item_tmp]\n",
    "    rated_users = train_pair[index_tmp][:,0]\n",
    "    rated_ratings = train_ratings[index_tmp]\n",
    "    if len(rated_users) == 0:\n",
    "        # if no rated users\n",
    "        pred_algo1[j] = glb_mean\n",
    "    else:\n",
    "        # sim_weight = S[user_tmp, rated_users].toarray()[0]\n",
    "        sim_weight = S[user_tmp, rated_users]\n",
    "        # print(sim_weight)\n",
    "        if max(sim_weight) == 0:\n",
    "            pred_algo1[j] = glb_mean\n",
    "        else:\n",
    "            pred_algo1[j] = np.sum(sim_weight*rated_ratings) / np.sum(sim_weight)"
   ]
  },
  {
   "source": [
    "### Summarize ALGO1 as a Python function\n",
    "\n",
    "- `Input`: 'train_pair', 'train_ratings', 'test_pair'\n",
    "\n",
    "- `Return`: 'pred_ratings'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim_user(u,v,train_pair,train_ratings):\n",
    "    index_u = np.where(train_pair[:,0] == u)[0]\n",
    "    index_v = np.where(train_pair[:,0] == v)[0]\n",
    "    item_u = train_pair[index_u][:,1]\n",
    "    item_v = train_pair[index_v][:,1]\n",
    "    # find co-rating items by `set`\n",
    "    item_co = list(set(item_u).intersection(set(item_v)))\n",
    "    if len(item_co) <= 0:\n",
    "        # a tuning parameter\n",
    "        return 0.0\n",
    "    else:\n",
    "        # find the co-rating vectors by using `np.isin`\n",
    "        vec_u, vec_v = train_ratings[index_u], train_ratings[index_v]\n",
    "        vec_co_u, vec_co_v = vec_u[np.isin(item_u, item_co)], vec_v[np.isin(item_v, item_co)]\n",
    "        return np.dot(vec_co_u, vec_co_v) / norm(vec_co_u) / norm(vec_co_v)\n",
    "\n",
    "def cor_RS_user(train_pair, train_ratings, test_pair):\n",
    "    n_user, n_item = train_pair[:,0].max()+1, train_pair[:,1].max()+1\n",
    "    ## Step 1: compute mean-centering ratings\n",
    "    MC_ratings = train_ratings.copy()\n",
    "    for u in range(n_user):\n",
    "        # find all indice for user-u use `np.where`\n",
    "        index_tmp = np.where(train_pair[:,0] == u)[0]\n",
    "        if len(index_tmp) < 2:\n",
    "            continue\n",
    "        MC_ratings[index_tmp] = MC_ratings[index_tmp] - MC_ratings[index_tmp].mean()\n",
    "    ## Step 2(a): compute similarity matrix\n",
    "    # dense sim_matrix\n",
    "    S = np.zeros((n_user,n_user))\n",
    "    # sparse matrix \n",
    "    # S = lil_matrix((n_user, n_user))\n",
    "    for u in range(n_user):\n",
    "        for v in range(u):\n",
    "            # S[u,v] = cossim_user(u,v,train_pair,train_ratings)\n",
    "            # use mean-centering ratings to compute the similarity\n",
    "            S[u,v] = cossim_user(u,v,train_pair,MC_ratings)\n",
    "    S = S + S.T\n",
    "    # Step 2(b): use `list` to store rated users for all items\n",
    "    index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "    ## Step 3: make prediction if S is stored\n",
    "    glb_mean = train_ratings.mean()\n",
    "    pred_algo1 = np.zeros(len(test_ratings))\n",
    "    for j in range(len(test_pair)):\n",
    "        user_tmp, item_tmp = test_pair[j,0], test_pair[j,1]\n",
    "        index_tmp = index_item[item_tmp]\n",
    "        rated_users = train_pair[index_tmp][:,0]\n",
    "        rated_ratings = train_ratings[index_tmp]\n",
    "        if len(rated_users) == 0:\n",
    "            # if no rated users\n",
    "            pred_algo1[j] = glb_mean\n",
    "        else:\n",
    "            # sim_weight = S[user_tmp, rated_users].toarray()[0]\n",
    "            sim_weight = S[user_tmp, rated_users]\n",
    "            # print(sim_weight)\n",
    "            if max(sim_weight) == 0:\n",
    "                pred_algo1[j] = glb_mean\n",
    "            else:\n",
    "                pred_algo1[j] = np.sum(sim_weight*rated_ratings) / np.sum(sim_weight)\n",
    "    return pred_algo1"
   ]
  },
  {
   "source": [
    "## Algo 2: Correlation-based (user-based) recommender systems (without stored sim-matrix)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_algo2 = np.zeros(len(test_ratings))\n",
    "\n",
    "## Updated Step 3: make prediction if S is stored\n",
    "glb_mean = train_ratings.mean()\n",
    "\n",
    "for u in range(n_user):\n",
    "    index_list_tmp = np.where(test_pair[:,0] == u)[0]\n",
    "    if len(index_list_tmp) == 0:\n",
    "        # no record to predict for this user.\n",
    "        continue\n",
    "    for record in index_list_tmp:\n",
    "        user_tmp, item_tmp = test_pair[record,0], test_pair[record,1]\n",
    "        index_tmp = index_item[item_tmp]\n",
    "        rated_users = train_pair[index_tmp][:,0]\n",
    "        rated_ratings = train_ratings[index_tmp]\n",
    "        if len(rated_users) == 0:\n",
    "            # if no rated users\n",
    "            pred_algo2[record] = glb_mean\n",
    "        else:\n",
    "            sim_weight = [cossim_user(user_tmp,v,train_pair,MC_ratings) for v in rated_ratings]\n",
    "            # print(sim_weight)\n",
    "            if max(sim_weight) == 0:\n",
    "                pred_algo2[record] = glb_mean\n",
    "            else:\n",
    "                pred_algo2[record] = np.sum(sim_weight*rated_ratings) / np.sum(sim_weight)"
   ]
  },
  {
   "source": [
    "## Baseline + correlation\n",
    "\n",
    "- Pre-processed ratings by Baseline methods.\n",
    "\n",
    "- Prediction by correlation-based algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Evaluation: compute RMSE for baseline methods "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for correlation-base RS (ALGO1): 2.588\n"
     ]
    }
   ],
   "source": [
    "## RMSE for ALGO1\n",
    "rmse_crs = np.sqrt(np.mean((pred_algo1 - test_ratings)**2))\n",
    "print('RMSE for correlation-base RS (ALGO1): %.3f' %rmse_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for correlation-base RS (ALGO2): 2.588\n"
     ]
    }
   ],
   "source": [
    "## RMSE for ALGO2\n",
    "rmse_crs_new = np.sqrt(np.mean((pred_algo2 - test_ratings)**2))\n",
    "print('RMSE for correlation-base RS (ALGO2): %.3f' %rmse_crs_new)"
   ]
  }
 ]
}