{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('shiing': conda)"
  },
  "interpreter": {
   "hash": "414c3711fdd48b2f544ed65b2d7540fc36858fd70ee4f390e2b75c8d3b465c57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook2: Correlation-based Recommender Systems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and pro-processed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dtrain = pd.read_csv('./dataset/train.csv')\n",
    "dtest = pd.read_csv('./dataset/test.csv')\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')\n",
    "\n",
    "## convert string to user_id and item_id -> [user_id, item_id, rating]\n",
    "# pre-process for training data\n",
    "train_pair = dtrain[['user_id', 'movie_id']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "# pre-process for testing set\n",
    "test_pair = dtest[['user_id', 'movie_id']].values\n",
    "\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implement Correlation-based (user-based) recommender systems\n",
    "- Inpout: training set.\n",
    "\n",
    "- Output: return predicted ratings for (user id, item id) user-item pairs in testing set.\n",
    "\n",
    "- Goal: make prediction for testing set\n",
    "\n",
    "- Note: time-space trade-off: [how to profiling and timing Code?](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algo: Correlation-based (user-based) recommender systems (stored sim-matrix by a sparse matrix)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Step 2(a): compute pairwise cosine similarity between all users\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cossim_user(index_u,index_v,train_pair,train_rating):\n",
    "    # index_u = np.where(train_pair[:,0] == u)[0]\n",
    "    # index_v = np.where(train_pair[:,0] == v)[0]\n",
    "    item_u = train_pair[index_u][:,1]\n",
    "    item_v = train_pair[index_v][:,1]\n",
    "    # find co-rating items by `set`\n",
    "    item_co = list(set(item_u).intersection(set(item_v)))\n",
    "    if len(item_co) < 3:\n",
    "        # a tuning parameter\n",
    "        return 0.0\n",
    "    else:\n",
    "        # find the co-rating vectors by using `np.isin`\n",
    "        vec_u, vec_v = train_rating[index_u], train_rating[index_v]\n",
    "        vec_co_u, vec_co_v = vec_u[np.isin(item_u, item_co)], vec_v[np.isin(item_v, item_co)]\n",
    "        return np.dot(vec_co_u, vec_co_v) / norm(vec_co_u) / norm(vec_co_v)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "pred_algo1 = np.zeros(len(test_rating))\n",
    "## Step 2(b): use `list` to store rated users and item for all items and user, respectively.\n",
    "index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "# dense sim_matrix\n",
    "# S = np.zeros((n_user,n_user))\n",
    "# sparse matrix \n",
    "S = lil_matrix((n_user, n_user))\n",
    "for u in range(n_user):\n",
    "    if u%500 == 0:\n",
    "        print('sim-vec for user_id %d' %u)\n",
    "    for v in range(u):\n",
    "        if (len(index_user[u]) == 0) or (len(index_user[u]) == 0):\n",
    "            continue\n",
    "        S[u,v] = cossim_user(index_user[u],index_user[v],train_pair,train_rating)\n",
    "S = S + S.T"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sim-vec for user_id 0\n",
      "sim-vec for user_id 500\n",
      "sim-vec for user_id 1000\n",
      "sim-vec for user_id 1500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "glb_mean = train_rating.mean()\n",
    "## Step 3: make prediction if S is stored\n",
    "for j in range(len(test_pair)):\n",
    "    user_tmp, item_tmp = test_pair[j,0], test_pair[j,1]\n",
    "    index_tmp = index_item[item_tmp]\n",
    "    rated_users = train_pair[index_tmp][:,0]\n",
    "    rated_ratings = train_rating[index_tmp]\n",
    "    sim_weight = S[user_tmp, rated_users].toarray()[0]\n",
    "    if (len(rated_users) == 0) or (max(sim_weight) == 0):\n",
    "        # if no rated users or no similar users\n",
    "        index_user_tmp = index_user[user_tmp]\n",
    "        if len(index_user_tmp) == 0:\n",
    "            pred_algo1[j] = glb_mean\n",
    "        else:\n",
    "            pred_algo1[j] = train_rating[index_user_tmp].mean()\n",
    "        \n",
    "        # find top 5 rated-users\n",
    "        # sim_weight_knn = np.zeros(len(sim_weight))\n",
    "        # top_index = np.argsort(sim_weight)[-5:]\n",
    "        # sim_weight_knn[top_index] = sim_weight[top_index]\n",
    "        # sim_weight = S[user_tmp, rated_users]\n",
    "        # print(sim_weight)\n",
    "    else:\n",
    "        pred_algo1[j] = np.sum(sim_weight*rated_ratings) / np.sum(sim_weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algo 2: Correlation-based (user-based) recommender systems (without stored sim-matrix)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pred_algo2 = np.zeros(len(test_rating))\n",
    "\n",
    "## Step 2(b): use `list` to store rated users and item for all items and user, respectively.\n",
    "index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "## Updated Step 3: make prediction if S is stored\n",
    "glb_mean = train_rating.mean()\n",
    "\n",
    "for user_tmp in range(n_user):\n",
    "    if user_tmp%500 == 0:\n",
    "        print('predict for user_id %d' %user_tmp)\n",
    "    index_user_tmp = np.where(test_pair[:,0] == user_tmp)[0]\n",
    "    train_index_user_tmp = index_user[user_tmp]\n",
    "    if len(index_user_tmp) == 0:\n",
    "        # no record to predict for this user.\n",
    "        continue\n",
    "    sim_weight = [cossim_user(index_user[user_tmp],index_user[v],train_pair,train_rating) for v in range(n_user)]\n",
    "    sim_weight = np.array(sim_weight)\n",
    "    for record in index_user_tmp:\n",
    "        item_tmp = test_pair[record,1]\n",
    "        index_item_tmp = index_item[item_tmp]\n",
    "        rated_users = train_pair[index_item_tmp][:,0]\n",
    "        rated_ratings = train_rating[index_item_tmp]\n",
    "        sim_weight_rated = sim_weight[rated_users]\n",
    "        if (len(rated_users) == 0) or (max(sim_weight_rated) == 0):\n",
    "            # if no rated users, then predict by user-mean\n",
    "            if len(train_index_user_tmp) == 0:\n",
    "                pred_algo2[record] = glb_mean\n",
    "            else:\n",
    "                pred_algo2[record] = train_rating[train_index_user_tmp].mean()\n",
    "        else:\n",
    "            pred_algo2[record] = np.sum(sim_weight_rated*rated_ratings) / np.sum(sim_weight_rated)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predict for user_id 0\n",
      "predict for user_id 500\n",
      "predict for user_id 1000\n",
      "predict for user_id 1500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline + correlation\n",
    "\n",
    "- Pre-processed ratings by Baseline methods.\n",
    "\n",
    "- Prediction by correlation-based algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation: compute RMSE for baseline methods "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## RMSE for ALGO1\n",
    "rmse_crs = np.sqrt(np.mean((pred_algo1 - test_rating)**2))\n",
    "print('RMSE for correlation-base RS (ALGO1): %.3f' %rmse_crs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for correlation-base RS (ALGO1): 1.053\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## RMSE for ALGO2\n",
    "rmse_crs_new = np.sqrt(np.mean((pred_algo2 - test_rating)**2))\n",
    "print('RMSE for correlation-base RS (ALGO2): %.3f' %rmse_crs_new)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for correlation-base RS (ALGO2): 1.053\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarize `correlation-based RS` as Python functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from numpy.linalg import norm\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def cossim_user(index_u,index_v,train_pair,train_rating):\n",
    "    # index_u = np.where(train_pair[:,0] == u)[0]\n",
    "    # index_v = np.where(train_pair[:,0] == v)[0]\n",
    "    item_u = train_pair[index_u][:,1]\n",
    "    item_v = train_pair[index_v][:,1]\n",
    "    # find co-rating items by `set`\n",
    "    item_co = list(set(item_u).intersection(set(item_v)))\n",
    "    if len(item_co) < 3:\n",
    "        # a tuning parameter\n",
    "        return 0.0\n",
    "    else:\n",
    "        # find the co-rating vectors by using `np.isin`\n",
    "        vec_u, vec_v = train_rating[index_u], train_rating[index_v]\n",
    "        vec_co_u, vec_co_v = vec_u[np.isin(item_u, item_co)], vec_v[np.isin(item_v, item_co)]\n",
    "        return np.dot(vec_co_u, vec_co_v) / norm(vec_co_u) / norm(vec_co_v)\n",
    "\n",
    "def cor_rs_user(train_pair, train_rating, test_pair):\n",
    "    n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1\n",
    "    index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "    index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "    pred = np.zeros(len(test_pair))\n",
    "    glb_mean = train_rating.mean()\n",
    "    # Step 1: compute sim-matrix\n",
    "    S = lil_matrix((n_user, n_user))\n",
    "    for u in range(n_user):\n",
    "        for v in range(u):\n",
    "            if (len(index_user[u]) == 0) or (len(index_user[v]) == 0):\n",
    "                continue\n",
    "            S[u,v] = cossim_user(index_user[u],index_user[v],train_pair,train_rating)\n",
    "    S = S + S.T\n",
    "    ## Step 3: make prediction if S is stored\n",
    "    for j in range(len(test_pair)):\n",
    "        user_tmp, item_tmp = test_pair[j,0], test_pair[j,1]\n",
    "        index_tmp = index_item[item_tmp]\n",
    "        rated_users = train_pair[index_tmp][:,0]\n",
    "        rated_ratings = train_rating[index_tmp]\n",
    "        sim_weight = S[user_tmp, rated_users].toarray()[0]\n",
    "        ## only keep top 5 closest users\n",
    "        top_ind = sim_weight.argsort()[-10:][::-1]\n",
    "        sim_weight_knn = np.zeros(len(sim_weight))\n",
    "        sim_weight_knn[top_ind] = sim_weight[top_ind]\n",
    "        if (len(rated_users) == 0) or (max(sim_weight_knn) == 0):\n",
    "            # if no rated users or no similar users\n",
    "            index_user_tmp = index_user[user_tmp]\n",
    "            if len(index_user_tmp) == 0:\n",
    "                pred[j] = glb_mean\n",
    "            else:\n",
    "                pred[j] = train_rating[index_user_tmp].mean()\n",
    "        else:\n",
    "            pred[j] = np.sum(sim_weight_knn*rated_ratings) / np.sum(sim_weight_knn)\n",
    "    return pred\n",
    "\n",
    "def cossim_item(index_i,index_j,train_pair,train_rating):\n",
    "    # index_u = np.where(train_pair[:,0] == u)[0]\n",
    "    # index_v = np.where(train_pair[:,0] == v)[0]\n",
    "    user_i = train_pair[index_i][:,0]\n",
    "    user_j = train_pair[index_j][:,0]\n",
    "    # find co-rating items by `set`\n",
    "    user_co = list(set(user_i).intersection(set(user_j)))\n",
    "    if len(user_co) < 3:\n",
    "        # a tuning parameter\n",
    "        return 0.0\n",
    "    else:\n",
    "        # find the co-rating vectors by using `np.isin`\n",
    "        vec_i, vec_j = train_rating[index_i], train_rating[index_j]\n",
    "        vec_co_i, vec_co_j = vec_i[np.isin(user_i, user_co)], vec_j[np.isin(user_j, user_co)]\n",
    "        return np.dot(vec_co_i, vec_co_j) / norm(vec_co_i) / norm(vec_co_j)\n",
    "\n",
    "def cor_rs_item(train_pair, train_rating, test_pair):\n",
    "    n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1\n",
    "    index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "    index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "    pred = np.zeros(len(test_pair))\n",
    "    glb_mean = train_rating.mean()\n",
    "    # Step 1: compute sim-matrix\n",
    "    S = lil_matrix((n_item, n_item))\n",
    "    for i in range(n_item):\n",
    "        for j in range(i):\n",
    "            if (len(index_item[i]) == 0) or (len(index_item[j]) == 0):\n",
    "                continue\n",
    "            S[i,j] = cossim_item(index_item[i],index_item[j],train_pair,train_rating)\n",
    "    S = S + S.T\n",
    "    ## Step 3: make prediction if S is stored\n",
    "    for j in range(len(test_pair)):\n",
    "        user_tmp, item_tmp = test_pair[j,0], test_pair[j,1]\n",
    "        index_tmp = index_item[item_tmp]\n",
    "        rated_items = train_pair[index_tmp][:,1]\n",
    "        rated_ratings = train_rating[index_tmp]\n",
    "        sim_weight = S[item_tmp, rated_items].toarray()[0]\n",
    "        ## only keep top 5 closest items\n",
    "        top_ind = sim_weight.argsort()[-5:][::-1]\n",
    "        sim_weight_knn = np.zeros(len(sim_weight))\n",
    "        sim_weight_knn[top_ind] = sim_weight[top_ind]\n",
    "        if (len(rated_items) == 0) or (max(sim_weight_knn) == 0):\n",
    "            # if no rated users or no similar users\n",
    "            index_item_tmp = index_item[item_tmp]\n",
    "            if len(index_item_tmp) == 0:\n",
    "                pred[j] = glb_mean\n",
    "            else:\n",
    "                pred[j] = train_rating[index_item_tmp].mean()\n",
    "        else:\n",
    "            pred[j] = np.sum(sim_weight_knn*rated_ratings) / np.sum(sim_weight_knn)\n",
    "    return pred"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}