{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('shiing': venv)"
  },
  "interpreter": {
   "hash": "0ba986c4ce28ee590feb069d7dff47f6c0fdeef1e6e7e0640650ca3a5af9b036"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook9: Smooth Recommender Systems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load the developed methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def rmse(true, pred):\n",
    "\treturn np.sqrt(np.mean((pred - true)**2))\n",
    "\n",
    "# baseline methods\n",
    "class glb_mean(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.glb_mean = 0\n",
    "\t\n",
    "\tdef fit(self, train_rating):\n",
    "\t\tself.glb_mean = np.mean(train_rating)\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))\n",
    "\t\tpred = pred*self.glb_mean\n",
    "\t\treturn pred\n",
    "\n",
    "class user_mean(object):\n",
    "\tdef __init__(self, n_user):\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.user_mean = np.zeros(n_user)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_rating):\n",
    "\t\tself.glb_mean = train_rating.mean()\n",
    "\t\tfor u in range(self.n_user):\n",
    "\t\t\tind_train = np.where(train_pair[:,0] == u)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.user_mean[u] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.user_mean[u] = train_rating[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.user_mean[user_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred\n",
    "\n",
    "class item_mean(object):\n",
    "\tdef __init__(self, n_item):\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.item_mean = np.zeros(n_item)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_rating):\n",
    "\t\tself.glb_mean = train_rating.mean()\n",
    "\t\tfor i in range(self.n_item):\n",
    "\t\t\tind_train = np.where(train_pair[:,1] == i)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.item_mean[i] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.item_mean[i] = train_rating[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.item_mean[item_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred\n",
    "\n",
    "\n",
    "class LFM(object):\n",
    "\n",
    "    def __init__(self, n_user, n_item, lam=.001, K=10, iterNum=10, tol=1e-4, verbose=1):\n",
    "        self.P = np.random.randn(n_user, K)\n",
    "        self.Q = np.random.randn(n_item, K)\n",
    "        # self.index_item = []\n",
    "        # self.index_user = []\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.lam = lam\n",
    "        self.K = K\n",
    "        self.iterNum = iterNum\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, train_pair, train_rating):\n",
    "        diff, tol = 1., self.tol\n",
    "        n_user, n_item, n_obs = self.n_user, self.n_item, len(train_pair)\n",
    "        K, iterNum, lam = self.K, self.iterNum, self.lam\n",
    "        ## store user/item index set\n",
    "        self.index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "        self.index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "        if self.verbose:\n",
    "            print('Fitting Reg-LFM: K: %d, lam: %.5f' %(K, lam))\n",
    "        for i in range(iterNum):\n",
    "            ## item update\n",
    "            score_old = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            for item_id in range(n_item):\n",
    "                index_item_tmp = self.index_item[item_id]\n",
    "                if len(index_item_tmp) == 0:\n",
    "                    self.Q[item_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_item_tmp:\n",
    "                    ## double-check\n",
    "                    if item_id != train_pair[record_ind][1]:\n",
    "                        raise ValueError('the item_id is waring in updating Q!')\n",
    "                    user_id, rating_tmp = train_pair[record_ind][0], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.P[user_id,:], self.P[user_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.P[user_id,:]                    \n",
    "                self.Q[item_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            \n",
    "            for user_id in range(n_user):\n",
    "                index_user_tmp = self.index_user[user_id]\n",
    "                if len(index_user_tmp) == 0:\n",
    "                    self.P[user_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_user_tmp:\n",
    "                    ## double-check\n",
    "                    if user_id != train_pair[record_ind][0]:\n",
    "                        raise ValueError('the user_id is waring in updating P!')\n",
    "                    item_id, rating_tmp = train_pair[record_ind][1], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.Q[item_id,:], self.Q[item_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.Q[item_id,:]                    \n",
    "                self.P[user_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            # compute the new rmse score\n",
    "            score_new = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            diff = abs(score_new - score_old) / score_old\n",
    "            if self.verbose:\n",
    "                print(\"Reg-LFM: ite: %d; diff: %.3f RMSE: %.3f\" %(i, diff, score_new))\n",
    "            if(diff < tol):\n",
    "                break\n",
    "\n",
    "    def predict(self, test_pair):\n",
    "        # predict ratings for user-item pairs\n",
    "        pred_rating = [np.dot(self.P[line[0]], self.Q[line[1]]) for line in test_pair]\n",
    "        return np.array(pred_rating)\n",
    "    \n",
    "    def rmse(self, test_pair, test_rating):\n",
    "        # report the rmse for the fitted `LFM`\n",
    "        pred_rating = self.predict(test_pair=test_pair)\n",
    "        return np.sqrt( np.mean( (pred_rating - test_rating)**2) )\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LFM_CV(object):\n",
    "\n",
    "\tdef __init__(self, n_user, n_item, cv=5,\n",
    "\t\t\t\tlams=[.000001,.0001,.001,.01], \n",
    "\t\t\t\tKs=[3,5,10,20], \n",
    "\t\t\t\titerNum=10, tol=1e-4):\n",
    "\t\t# self.index_item = []\n",
    "\t\t# self.index_user = []\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.cv = cv\n",
    "\t\tself.lams = lams\n",
    "\t\tself.Ks = Ks\n",
    "\t\tself.iterNum = iterNum\n",
    "\t\tself.tol = tol\n",
    "\t\tself.best_model = {}\n",
    "\t\tself.cv_result = {'K': [], 'lam': [], 'train_rmse': [], 'valid_rmse': []}\n",
    "\n",
    "\tdef grid_search(self, train_pair, train_rating):\n",
    "\t\t## generate all comb of `K` and `lam`\n",
    "\t\tkf = KFold(n_splits=self.cv, shuffle=True)\n",
    "\t\tfor (K,lam) in itertools.product(self.Ks, self.lams):\n",
    "\t\t\ttrain_rmse_tmp, valid_rmse_tmp = 0., 0.\n",
    "\t\t\tfor train_index, valid_index in kf.split(train_pair):\n",
    "\t\t\t\t# produce training/validation sets\n",
    "\t\t\t\ttrain_pair_cv, train_rating_cv = train_pair[train_index], train_rating[train_index]\n",
    "\t\t\t\tvalid_pair_cv, valid_rating_cv = train_pair[valid_index], train_rating[valid_index]\n",
    "\t\t\t\t# fit the model based on CV data\n",
    "\t\t\t\tmodel_tmp = LFM(self.n_user, self.n_item, K=K, lam=lam, verbose=0)\n",
    "\t\t\t\tmodel_tmp.fit(train_pair=train_pair_cv, train_rating=train_rating_cv)\n",
    "\t\t\t\ttrain_rmse_tmp_cv = model_tmp.rmse(test_pair=train_pair_cv, test_rating=train_rating_cv)\n",
    "\t\t\t\tvalid_rmse_tmp_cv = model_tmp.rmse(test_pair=valid_pair_cv, test_rating=valid_rating_cv)\n",
    "\t\t\t\ttrain_rmse_tmp = train_rmse_tmp + train_rmse_tmp_cv / self.cv\n",
    "\t\t\t\tvalid_rmse_tmp = valid_rmse_tmp + valid_rmse_tmp_cv / self.cv\n",
    "\t\t\t\tprint('%d-Fold CV for K: %d; lam: %.5f: train_rmse: %.3f, valid_rmse: %.3f' \n",
    "\t\t\t\t\t\t%(self.cv, K, lam, train_rmse_tmp_cv, valid_rmse_tmp_cv))\n",
    "\t\t\tself.cv_result['K'].append(K)\n",
    "\t\t\tself.cv_result['lam'].append(lam)\n",
    "\t\t\tself.cv_result['train_rmse'].append(train_rmse_tmp)\n",
    "\t\t\tself.cv_result['valid_rmse'].append(valid_rmse_tmp)\n",
    "\t\tself.cv_result = pd.DataFrame.from_dict(self.cv_result)\n",
    "\t\tbest_ind = self.cv_result['valid_rmse'].argmin()\n",
    "\t\tself.best_model = self.cv_result.loc[best_ind]\n",
    "\t\n",
    "\tdef plot_grid(self, data_source='valid'):\n",
    "\t\tsns.set_theme()\n",
    "\t\tif data_source == 'train':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"train_rmse\")\n",
    "\t\telif data_source == 'valid':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"valid_rmse\")\n",
    "\t\telse:\n",
    "\t\t\traise ValueError('data_source must be train or valid!')\n",
    "\t\tsns.heatmap(cv_pivot, annot=True, fmt=\".3f\", linewidths=.5, cmap=\"YlGnBu\")\n",
    "\t\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dtrain = pd.read_csv('./dataset/train.csv')\n",
    "dtest = pd.read_csv('./dataset/test.csv')\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')\n",
    "\n",
    "## convert string to user_id and item_id -> [user_id, item_id, rating]\n",
    "# pre-process for training data\n",
    "train_pair = dtrain[['user_id', 'movie_id']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "# pre-process for testing set\n",
    "test_pair = dtest[['user_id', 'movie_id']].values\n",
    "\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define and training the predictive models based on `class`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## baseline user mean methods\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_rating=train_rating)\n",
    "pred_user = user_ave.predict(test_pair)\n",
    "print('RMSE for user_mean: %.3f' %rmse(test_rating, pred_user) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## baseline item mean methods\n",
    "item_ave = item_mean(n_item=n_item)\n",
    "item_ave.fit(train_pair=train_pair, train_rating=train_rating)\n",
    "pred_item = item_ave.predict(test_pair)\n",
    "print('RMSE for item_mean: %.3f' %rmse(test_rating, pred_item) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## CV based on `LFM_CV`\n",
    "## Baseline + LFM\n",
    "glb_ave = glb_mean()\n",
    "glb_ave.fit(train_rating)\n",
    "pred = glb_ave.predict(test_pair)\n",
    "# user_mean\n",
    "train_rating_cm = train_rating - glb_ave.predict(train_pair)\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_rating=train_rating_cm)\n",
    "train_rating_res = train_rating_cm - user_ave.predict(train_pair)\n",
    "dtrain['res_rating'] = train_rating_res\n",
    "pred = pred + user_ave.predict(test_pair)\n",
    "# fit LFM_CV by residual ratings \n",
    "Ks, lams = [2, 3, 5, 10], 10**np.arange(-6, -2, .5)\n",
    "shiing_cv = LFM_CV(n_user, n_item, cv=3, Ks=Ks, lams=lams)\n",
    "shiing_cv.grid_search(train_pair, train_rating_res)\n",
    "shiing_cv.plot_grid('valid')\n",
    "shiing_cv.plot_grid('train')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## refit the best model, and make prediction\n",
    "best_K, best_lam = int(shiing_cv.best_model['K']), shiing_cv.best_model['lam']\n",
    "print('best K: %d, best lam: %.5f' %(best_K, best_lam))\n",
    "shiing=LFM(n_user, n_item, K=best_K, lam=best_lam)\n",
    "shiing.fit(train_pair, train_rating_res)\n",
    "pred = pred + shiing.predict(test_pair)\n",
    "print('RMSE for glb + user_mean + LFM: %.3f' %rmse(test_rating, pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Smooth Recommender Systems\n",
    "## Step 1: Generate the side info for users and items"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "user_info = pd.DataFrame({'user_id': list(range(n_user))})\n",
    "user_info = user_info.set_index('user_id')\n",
    "user_info['mean'] = dtrain.groupby('user_id')['res_rating'].mean()\n",
    "user_info['q1'] = dtrain.groupby('user_id')['res_rating'].quantile(.1)\n",
    "user_info['q3'] = dtrain.groupby('user_id')['res_rating'].quantile(.3)\n",
    "user_info['q5'] = dtrain.groupby('user_id')['res_rating'].quantile(.5)\n",
    "user_info['q7'] = dtrain.groupby('user_id')['res_rating'].quantile(.7)\n",
    "user_info['q7'] = dtrain.groupby('user_id')['res_rating'].quantile(.9)\n",
    "## fill NAN as the column mean\n",
    "user_info = user_info.fillna(user_info.mean())\n",
    "user_scaler = StandardScaler()\n",
    "user_info = user_scaler.fit_transform(user_info)\n",
    "\n",
    "movie_info = pd.DataFrame({'movie_id': list(range(n_item))})\n",
    "movie_info = movie_info.set_index('movie_id')\n",
    "movie_info['mean'] = dtrain.groupby('movie_id')['res_rating'].mean()\n",
    "movie_info['q1'] = dtrain.groupby('movie_id')['res_rating'].quantile(.1)\n",
    "movie_info['q3'] = dtrain.groupby('movie_id')['res_rating'].quantile(.3)\n",
    "movie_info['q5'] = dtrain.groupby('movie_id')['res_rating'].quantile(.5)\n",
    "movie_info['q7'] = dtrain.groupby('movie_id')['res_rating'].quantile(.7)\n",
    "movie_info['q7'] = dtrain.groupby('movie_id')['res_rating'].quantile(.9)\n",
    "## fill NAN as the column mean\n",
    "movie_info = movie_info.fillna(movie_info.mean())\n",
    "movie_scaler = StandardScaler()\n",
    "movie_info = movie_scaler.fit_transform(movie_info)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Weight matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "user_sim = cosine_similarity(user_info)\n",
    "movie_sim = cosine_similarity(movie_info)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Compute the augmented dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top = 10\n",
    "index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "## augmented data\n",
    "fake_pair, fake_rating = [], []\n",
    "for u in range(n_user):\n",
    "\tprint('UserId: %d' %u)\n",
    "\ttop_user_tmp = user_sim[u].argsort()[-top:][::-1]\n",
    "\tvalid_user_ind = []\n",
    "\tfor u_tmp in top_user_tmp:\n",
    "\t\tvalid_user_ind.extend(index_user[u_tmp])\n",
    "\tobs_movie_tmp = train_pair[valid_user_ind,1]\n",
    "\tfor i in range(n_item):\n",
    "\t\ttop_movie_tmp = movie_sim[i].argsort()[-top:][::-1]\n",
    "\t\tvalid_movie_tmp = np.intersect1d(top_movie_tmp, obs_movie_tmp)\n",
    "\t\tif len(valid_movie_tmp) == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tvalid_item_ind = []\n",
    "\t\tfor i_tmp in valid_movie_tmp:\n",
    "\t\t\tvalid_item_ind.extend(index_item[i_tmp])\n",
    "\t\tvalid_ind = np.intersect1d(valid_user_ind, valid_item_ind)\n",
    "\t\tif len(valid_ind) > 0:\n",
    "\t\t\tfake_pair.append([u,i])\n",
    "\t\t\tfake_rating.append(train_rating_res[valid_ind].mean())\n",
    "fake_pair, fake_rating = np.array(fake_pair), np.array(fake_rating)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Fit a Latent Factor Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aug_pair, aug_rating_res = np.vstack((train_pair, fake_pair)), np.hstack((train_rating_res, fake_rating))\n",
    "\n",
    "Ks, lams = [2, 3, 5, 10], 10**np.arange(-6, -2, .5)\n",
    "sSVD_cv = LFM_CV(n_user, n_item, cv=3, Ks=Ks, lams=lams)\n",
    "sSVD_cv.grid_search(aug_pair, aug_rating)\n",
    "sSVD_cv.plot_grid('valid')\n",
    "sSVD_cv.plot_grid('train')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}