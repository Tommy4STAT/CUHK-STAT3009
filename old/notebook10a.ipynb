{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook10(a): binary recommender systems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process the ML-100K raw data\n",
    "- check the `user_id` and `item_id`: mapping `item_id` to a continuous sequence based on `sklean.preprocessing`\n",
    "- use `sklearn.model_selection.train_test_split` to generate `train` and `test` dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load additional ``side information``\n",
    "\n",
    "ref: https://colab.research.google.com/github/lcharlin/80-629/blob/master/week4-PracticalSession/Introduction_to_ML.ipynb#scrollTo=4R717-S52plZ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# load rating\n",
    "df = pd.read_csv('./dataset/ml-latest-small/ratings.csv')\n",
    "del df['timestamp']\n",
    "\n",
    "movies_pd = pd.read_csv('./dataset/ml-latest-small/movies.csv', sep=',', engine='python')\n",
    "movies_pd.sample(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      movieId                           title                genres\n",
       "2840     3798        What Lies Beneath (2000)  Drama|Horror|Mystery\n",
       "1189     1586                G.I. Jane (1997)          Action|Drama\n",
       "116       141            Birdcage, The (1996)                Comedy\n",
       "6678    57843  Rise of the Footsoldier (2007)    Action|Crime|Drama\n",
       "4994     7707        He Said, She Said (1991)  Comedy|Drama|Romance\n",
       "9242   154975           Merci Patron ! (2016)    Comedy|Documentary\n",
       "7860    93855        God Bless America (2011)          Comedy|Drama\n",
       "4383     6427    Railway Children, The (1970)        Children|Drama\n",
       "8481   113186                   Felony (2013)              Thriller\n",
       "2668     3571                Time Code (2000)          Comedy|Drama"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>3798</td>\n",
       "      <td>What Lies Beneath (2000)</td>\n",
       "      <td>Drama|Horror|Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1586</td>\n",
       "      <td>G.I. Jane (1997)</td>\n",
       "      <td>Action|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>141</td>\n",
       "      <td>Birdcage, The (1996)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>57843</td>\n",
       "      <td>Rise of the Footsoldier (2007)</td>\n",
       "      <td>Action|Crime|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>7707</td>\n",
       "      <td>He Said, She Said (1991)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>154975</td>\n",
       "      <td>Merci Patron ! (2016)</td>\n",
       "      <td>Comedy|Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>93855</td>\n",
       "      <td>God Bless America (2011)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>6427</td>\n",
       "      <td>Railway Children, The (1970)</td>\n",
       "      <td>Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8481</th>\n",
       "      <td>113186</td>\n",
       "      <td>Felony (2013)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>3571</td>\n",
       "      <td>Time Code (2000)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering\n",
    "- extract `year` and `genre` from the movies' side information\n",
    "- For simplicity, if multiple genres exist, we just take the first one\n",
    "- `Regex` to deal with the raw data [tutorial](https://regexone.com/) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import re\n",
    "\n",
    "year, genre = [], []\n",
    "for i in range(len(movies_pd)):\n",
    "\trow = movies_pd.loc[i]\n",
    "\tyear_tmp = re.findall('\\d+', row['title'])\n",
    "\tif len(year_tmp) > 0:\n",
    "\t\tyear.append(int(year_tmp[0]))\n",
    "\telse:\n",
    "\t\tyear.append(np.nan)\n",
    "\t## take the first one as primary genere\n",
    "\tgenre.append(row['genres'].split('|')[0])\n",
    "\n",
    "movies_pd['year'], movies_pd['pGenre'] = year, genre\n",
    "## delete original title and genres\n",
    "del movies_pd['title']\n",
    "del movies_pd['genres']\n",
    "movies_pd.sample(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      movieId    year     pGenre\n",
       "5866    32875  1949.0     Comedy\n",
       "2209     2936  1941.0  Adventure\n",
       "1473     1998  1977.0     Horror\n",
       "2783     3724  1978.0      Drama\n",
       "9633   179133  2017.0  Animation\n",
       "849      1119  1995.0      Drama\n",
       "8989   139157  2015.0     Comedy\n",
       "2059     2738  1986.0     Comedy\n",
       "2545     3406  1951.0     Action\n",
       "2084     2770  1999.0     Comedy"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>year</th>\n",
       "      <th>pGenre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>32875</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>2936</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>1998</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>3724</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9633</th>\n",
       "      <td>179133</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>1119</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8989</th>\n",
       "      <td>139157</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>2738</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>3406</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>2770</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Missing data](https://machinelearningmastery.com/handle-missing-data-python/)\n",
    "- Usually we impute the missing values by average, but there are some fancy methods, see [Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html#impute).\n",
    "- Use package `sklearn.impute.SimpleImputer`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(movies_pd['year'].values.reshape(-1, 1))\n",
    "movies_pd['year'] = imp_mean.transform(movies_pd['year'].values.reshape(-1, 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate some additional side information for users and items\n",
    "- Number of ratings\n",
    "- Averaged ratings\n",
    "- quantiles of the ratings (as a practice)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "user_pd = pd.merge(left=df.groupby('userId')['rating'].mean(), \n",
    "\t\t\t\t\tright=df.groupby('userId')['rating'].count(), on='userId', )\n",
    "user_pd.columns = ['rating_mean', 'rating_count']\n",
    "user_pd = user_pd.reset_index()\n",
    "\n",
    "movie_rating_pd = pd.merge(left=df.groupby('movieId')['rating'].mean(), \n",
    "\t\t\t\t\t\tright=df.groupby('movieId')['rating'].count(), on='movieId')\n",
    "movie_rating_pd.columns\t= ['rating_mean', 'rating_count']\n",
    "\n",
    "movies_pd = pd.merge(left=movie_rating_pd, right=movies_pd, on='movieId')\n",
    "\n",
    "print(user_pd.sample(10))\n",
    "print(movies_pd.sample(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     userId  rating_mean  rating_count\n",
      "587     588     3.250000            56\n",
      "11       12     4.390625            32\n",
      "488     489     3.017747           648\n",
      "243     244     3.774194            93\n",
      "235     236     3.966667            30\n",
      "96       97     4.194444            36\n",
      "551     552     3.119681           188\n",
      "597     598     3.809524            21\n",
      "593     594     3.924569           232\n",
      "347     348     4.672727            55\n",
      "      movieId  rating_mean  rating_count    year       pGenre\n",
      "3288     4453     3.500000             1  2000.0  Documentary\n",
      "9282   158956     2.500000             2  2016.0       Action\n",
      "7331    78218     3.000000             1  2010.0        Drama\n",
      "3862     5437     3.000000             3  1986.0       Comedy\n",
      "939      1241     4.050000            10  1992.0       Comedy\n",
      "4351     6374     2.166667             3  2003.0       Comedy\n",
      "7189    72733     3.857143             7  2009.0        Drama\n",
      "2409     3201     4.500000             8  1970.0        Drama\n",
      "3567     4890     3.032258            31  2001.0       Comedy\n",
      "146       174     2.700000            10  1995.0       Comedy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-processing the dataset\n",
    "- all continuous features should be standardized as mean 0, std 1\n",
    "- all categorical features should be re-encoding to remove the missing ones"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn import preprocessing\n",
    "## pre-processing for users\n",
    "user_cont = ['rating_mean', 'rating_count']\n",
    "user_pd[user_cont] = preprocessing.StandardScaler().fit_transform(user_pd[user_cont])\n",
    "\n",
    "## pre-processing for movies\n",
    "movie_cont = ['rating_mean', 'rating_count', 'year']\n",
    "movies_pd[movie_cont] = preprocessing.StandardScaler().fit_transform(movies_pd[movie_cont])\n",
    "\n",
    "## encoding for categorical data \n",
    "from sklearn import preprocessing\n",
    "le_genre = preprocessing.LabelEncoder()\n",
    "movies_pd['pGenre'] = le_genre.fit_transform(movies_pd['pGenre'])\n",
    "\n",
    "## joint encoding for userId and movieId\n",
    "# !!! all dfs should share the same encoding for userId and movieId, respecitively!!!\n",
    "le_movie = preprocessing.LabelEncoder()\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "\n",
    "df['movieId'] = le_movie.fit_transform(df['movieId'])\n",
    "df['userId'] = le_user.fit_transform(df['userId'])\n",
    "\n",
    "movies_pd['movieId'] = le_movie.transform(movies_pd['movieId'])\n",
    "user_pd['userId'] = le_user.transform(user_pd['userId'])\n",
    "\n",
    "user_pd = user_pd.set_index('userId', drop=False)\n",
    "movies_pd = movies_pd.set_index('movieId', drop=False)\n",
    "## generate train / test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dtest = train_test_split(df, test_size=0.33, random_state=42)\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# tran_pair, train_rating\n",
    "train_pair = dtrain[['userId', 'movieId']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "# test_pair\n",
    "test_pair = dtest[['userId', 'movieId']].values\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate a binary dataset\n",
    "- if `rating` >= 3.5: ``LIKE``;\n",
    "- if `rating` <3.5: ``DISLIKE``\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_like = 1*(train_rating >= 3.5)\n",
    "test_like = 1*(test_rating >= 3.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the existing methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def rmse(true, pred):\n",
    "\treturn np.sqrt(np.mean((pred - true)**2))\n",
    "\n",
    "# baseline methods\n",
    "class glb_mean(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.glb_mean = 0\n",
    "\t\n",
    "\tdef fit(self, train_ratings):\n",
    "\t\tself.glb_mean = np.mean(train_ratings)\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))\n",
    "\t\tpred = pred*self.glb_mean\n",
    "\t\treturn pred\n",
    "\n",
    "class user_mean(object):\n",
    "\tdef __init__(self, n_user):\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.user_mean = np.zeros(n_user)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_ratings):\n",
    "\t\tself.glb_mean = train_ratings.mean()\n",
    "\t\tfor u in range(self.n_user):\n",
    "\t\t\tind_train = np.where(train_pair[:,0] == u)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.user_mean[u] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.user_mean[u] = train_ratings[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.user_mean[user_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred\n",
    "\n",
    "class item_mean(object):\n",
    "\tdef __init__(self, n_item):\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.item_mean = np.zeros(n_item)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_ratings):\n",
    "\t\tself.glb_mean = train_ratings.mean()\n",
    "\t\tfor i in range(self.n_item):\n",
    "\t\t\tind_train = np.where(train_pair[:,1] == i)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.item_mean[i] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.item_mean[i] = train_ratings[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.item_mean[item_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred\n",
    "\n",
    "\n",
    "class LFM(object):\n",
    "\n",
    "    def __init__(self, n_user, n_item, lam=.001, K=10, iterNum=10, tol=1e-4, verbose=1):\n",
    "        self.P = np.random.randn(n_user, K)\n",
    "        self.Q = np.random.randn(n_item, K)\n",
    "        # self.index_item = []\n",
    "        # self.index_user = []\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.lam = lam\n",
    "        self.K = K\n",
    "        self.iterNum = iterNum\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, train_pair, train_rating):\n",
    "        diff, tol = 1., self.tol\n",
    "        n_user, n_item, n_obs = self.n_user, self.n_item, len(train_pair)\n",
    "        K, iterNum, lam = self.K, self.iterNum, self.lam\n",
    "        ## store user/item index set\n",
    "        self.index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "        self.index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "        if self.verbose:\n",
    "            print('Fitting Reg-LFM: K: %d, lam: %.5f' %(K, lam))\n",
    "        for i in range(iterNum):\n",
    "            ## item update\n",
    "            score_old = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            for item_id in range(n_item):\n",
    "                index_item_tmp = self.index_item[item_id]\n",
    "                if len(index_item_tmp) == 0:\n",
    "                    self.Q[item_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_item_tmp:\n",
    "                    ## double-check\n",
    "                    if item_id != train_pair[record_ind][1]:\n",
    "                        raise ValueError('the item_id is waring in updating Q!')\n",
    "                    user_id, rating_tmp = train_pair[record_ind][0], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.P[user_id,:], self.P[user_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.P[user_id,:]                    \n",
    "                self.Q[item_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            \n",
    "            for user_id in range(n_user):\n",
    "                index_user_tmp = self.index_user[user_id]\n",
    "                if len(index_user_tmp) == 0:\n",
    "                    self.P[user_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_user_tmp:\n",
    "                    ## double-check\n",
    "                    if user_id != train_pair[record_ind][0]:\n",
    "                        raise ValueError('the user_id is waring in updating P!')\n",
    "                    item_id, rating_tmp = train_pair[record_ind][1], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.Q[item_id,:], self.Q[item_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.Q[item_id,:]                    \n",
    "                self.P[user_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            # compute the new rmse score\n",
    "            score_new = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            diff = abs(score_new - score_old) / score_old\n",
    "            if self.verbose:\n",
    "                print(\"Reg-LFM: ite: %d; diff: %.3f RMSE: %.3f\" %(i, diff, score_new))\n",
    "            if(diff < tol):\n",
    "                break\n",
    "\n",
    "    def predict(self, test_pair):\n",
    "        # predict ratings for user-item pairs\n",
    "        pred_rating = [np.dot(self.P[line[0]], self.Q[line[1]]) for line in test_pair]\n",
    "        return np.array(pred_rating)\n",
    "    \n",
    "    def rmse(self, test_pair, test_rating):\n",
    "        # report the rmse for the fitted `LFM`\n",
    "        pred_rating = self.predict(test_pair=test_pair)\n",
    "        return np.sqrt( np.mean( (pred_rating - test_rating)**2) )\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LFM_CV(object):\n",
    "\n",
    "\tdef __init__(self, n_user, n_item, cv=5,\n",
    "\t\t\t\tlams=[.000001,.0001,.001,.01], \n",
    "\t\t\t\tKs=[3,5,10,20], \n",
    "\t\t\t\titerNum=10, tol=1e-4):\n",
    "\t\t# self.index_item = []\n",
    "\t\t# self.index_user = []\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.cv = cv\n",
    "\t\tself.lams = lams\n",
    "\t\tself.Ks = Ks\n",
    "\t\tself.iterNum = iterNum\n",
    "\t\tself.tol = tol\n",
    "\t\tself.best_model = {}\n",
    "\t\tself.cv_result = {'K': [], 'lam': [], 'train_rmse': [], 'valid_rmse': []}\n",
    "\n",
    "\tdef grid_search(self, train_pair, train_rating):\n",
    "\t\t## generate all comb of `K` and `lam`\n",
    "\t\tkf = KFold(n_splits=self.cv, shuffle=True)\n",
    "\t\tfor (K,lam) in itertools.product(self.Ks, self.lams):\n",
    "\t\t\ttrain_rmse_tmp, valid_rmse_tmp = 0., 0.\n",
    "\t\t\tfor train_index, valid_index in kf.split(train_pair):\n",
    "\t\t\t\t# produce training/validation sets\n",
    "\t\t\t\ttrain_pair_cv, train_rating_cv = train_pair[train_index], train_rating[train_index]\n",
    "\t\t\t\tvalid_pair_cv, valid_rating_cv = train_pair[valid_index], train_rating[valid_index]\n",
    "\t\t\t\t# fit the model based on CV data\n",
    "\t\t\t\tmodel_tmp = LFM(self.n_user, self.n_item, K=K, lam=lam, verbose=0)\n",
    "\t\t\t\tmodel_tmp.fit(train_pair=train_pair_cv, train_rating=train_rating_cv)\n",
    "\t\t\t\ttrain_rmse_tmp_cv = model_tmp.rmse(test_pair=train_pair_cv, test_rating=train_rating_cv)\n",
    "\t\t\t\tvalid_rmse_tmp_cv = model_tmp.rmse(test_pair=valid_pair_cv, test_rating=valid_rating_cv)\n",
    "\t\t\t\ttrain_rmse_tmp = train_rmse_tmp + train_rmse_tmp_cv / self.cv\n",
    "\t\t\t\tvalid_rmse_tmp = valid_rmse_tmp + valid_rmse_tmp_cv / self.cv\n",
    "\t\t\t\tprint('%d-Fold CV for K: %d; lam: %.5f: train_rmse: %.3f, valid_rmse: %.3f' \n",
    "\t\t\t\t\t\t%(self.cv, K, lam, train_rmse_tmp_cv, valid_rmse_tmp_cv))\n",
    "\t\t\tself.cv_result['K'].append(K)\n",
    "\t\t\tself.cv_result['lam'].append(lam)\n",
    "\t\t\tself.cv_result['train_rmse'].append(train_rmse_tmp)\n",
    "\t\t\tself.cv_result['valid_rmse'].append(valid_rmse_tmp)\n",
    "\t\tself.cv_result = pd.DataFrame.from_dict(self.cv_result)\n",
    "\t\tbest_ind = self.cv_result['valid_rmse'].argmin()\n",
    "\t\tself.best_model = self.cv_result.loc[best_ind]\n",
    "\t\n",
    "\tdef plot_grid(self, data_source='valid'):\n",
    "\t\tsns.set_theme()\n",
    "\t\tif data_source == 'train':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"train_rmse\")\n",
    "\t\telif data_source == 'valid':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"valid_rmse\")\n",
    "\t\telse:\n",
    "\t\t\traise ValueError('data_source must be train or valid!')\n",
    "\t\tsns.heatmap(cv_pivot, annot=True, fmt=\".3f\", linewidths=.5, cmap=\"YlGnBu\")\n",
    "\t\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ``NCF`` Model based on side information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Formulate neural network based on continuous and categorical features\n",
    "- embedding for categorical features\n",
    "- concatenate continuous features and all embedding vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import SVG\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-03 16:51:04.535519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:04.535537: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class SideNCF(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, num_genre, embedding_size, **kwargs):\n",
    "        super(SideNCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.genre_embedding = layers.Embedding(\n",
    "            num_genre,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(100, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(50, name='fc-2', activation='relu')\n",
    "        ## we need to change the last layer activation function as sigmiod\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        cont_feats = inputs[0]\n",
    "        cate_feats = inputs[1]\n",
    "        user_vector = self.user_embedding(cate_feats[:,0])\n",
    "        movie_vector = self.movie_embedding(cate_feats[:,1])\n",
    "        genre_vector = self.genre_embedding(cate_feats[:,2])\n",
    "        concatted_vec = self.concatenate([cont_feats, user_vector, movie_vector, genre_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ``loss`` function and evaluation ``metrics`` should be changed!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "num_genre = movies_pd['pGenre'].max() + 1\n",
    "model = SideNCF(num_users=n_user, num_movies=n_item, num_genre=num_genre, embedding_size=50)\n",
    "\n",
    "# metrics = [\n",
    "#     keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "#     keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "# ]\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(1e-3), \n",
    "#     loss=tf.keras.losses.MeanSquaredError(), \n",
    "#     metrics=metrics\n",
    "# )\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.BinaryAccuracy(name='binary_acc')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3), \n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-03 16:51:13.421643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-03 16:51:13.421931: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.421980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422233: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:51:13.422241: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-09-03 16:51:13.422492: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: produce the continuous and categorical features for users and items"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "## find the continuous features and categorical features for user and item, respectively\n",
    "movie_cont, movie_cate = ['rating_mean', 'rating_count'], ['movieId', 'pGenre']\n",
    "user_cont, user_cate = ['rating_mean', 'rating_count'], ['userId']\n",
    "\n",
    "train_cont_feats = np.hstack((user_pd.loc[train_pair[:,0]][user_cont], movies_pd.loc[train_pair[:,1]][movie_cont]))\n",
    "train_cate_feats = np.hstack((user_pd.loc[train_pair[:,0]][user_cate], movies_pd.loc[train_pair[:,1]][movie_cate]))\n",
    "\n",
    "test_cont_feats = np.hstack((user_pd.loc[test_pair[:,0]][user_cont], movies_pd.loc[test_pair[:,1]][movie_cont]))\n",
    "test_cate_feats = np.hstack((user_pd.loc[test_pair[:,0]][user_cate], movies_pd.loc[test_pair[:,1]][movie_cate]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Feed neural network with multi-source dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_binary_acc', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[train_cont_feats, train_cate_feats],\n",
    "    y=train_like,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=.2,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-03 16:51:23.917633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.7898 - binary_acc: 0.7590 - val_loss: 0.5140 - val_binary_acc: 0.7632\n",
      "Epoch 2/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4975 - binary_acc: 0.7659 - val_loss: 0.5037 - val_binary_acc: 0.7606\n",
      "Epoch 3/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4959 - binary_acc: 0.7679 - val_loss: 0.5064 - val_binary_acc: 0.7587\n",
      "Epoch 4/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4945 - binary_acc: 0.7684 - val_loss: 0.5022 - val_binary_acc: 0.7632\n",
      "Epoch 5/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4926 - binary_acc: 0.7699 - val_loss: 0.5030 - val_binary_acc: 0.7630\n",
      "Epoch 6/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4921 - binary_acc: 0.7712 - val_loss: 0.5013 - val_binary_acc: 0.7607\n",
      "Epoch 7/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4914 - binary_acc: 0.7708 - val_loss: 0.5001 - val_binary_acc: 0.7627\n",
      "Epoch 8/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4892 - binary_acc: 0.7713 - val_loss: 0.4977 - val_binary_acc: 0.7652\n",
      "Epoch 9/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4905 - binary_acc: 0.7715 - val_loss: 0.4968 - val_binary_acc: 0.7627\n",
      "Epoch 10/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4900 - binary_acc: 0.7727 - val_loss: 0.5006 - val_binary_acc: 0.7651\n",
      "Epoch 11/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4907 - binary_acc: 0.7737 - val_loss: 0.4998 - val_binary_acc: 0.7635\n",
      "Epoch 12/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4896 - binary_acc: 0.7722 - val_loss: 0.5013 - val_binary_acc: 0.7651\n",
      "Epoch 13/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4883 - binary_acc: 0.7734 - val_loss: 0.4978 - val_binary_acc: 0.7652\n",
      "Epoch 14/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4874 - binary_acc: 0.7735 - val_loss: 0.5014 - val_binary_acc: 0.7649\n",
      "Epoch 15/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4881 - binary_acc: 0.7738 - val_loss: 0.4987 - val_binary_acc: 0.7658\n",
      "Epoch 16/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4875 - binary_acc: 0.7752 - val_loss: 0.4982 - val_binary_acc: 0.7647\n",
      "Epoch 17/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4871 - binary_acc: 0.7747 - val_loss: 0.4967 - val_binary_acc: 0.7681\n",
      "Epoch 18/50\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 0.4860 - binary_acc: 0.7756 - val_loss: 0.4991 - val_binary_acc: 0.7678\n",
      "Epoch 19/50\n",
      "845/845 [==============================] - 4s 5ms/step - loss: 0.4857 - binary_acc: 0.7746 - val_loss: 0.4983 - val_binary_acc: 0.7658\n",
      "Epoch 20/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4866 - binary_acc: 0.7746 - val_loss: 0.5007 - val_binary_acc: 0.7664\n",
      "Epoch 21/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4863 - binary_acc: 0.7762 - val_loss: 0.5003 - val_binary_acc: 0.7694\n",
      "Epoch 22/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4864 - binary_acc: 0.7767 - val_loss: 0.4978 - val_binary_acc: 0.7664\n",
      "Epoch 23/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4861 - binary_acc: 0.7762 - val_loss: 0.4994 - val_binary_acc: 0.7663\n",
      "Epoch 24/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4850 - binary_acc: 0.7756 - val_loss: 0.4973 - val_binary_acc: 0.7679\n",
      "Epoch 25/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4842 - binary_acc: 0.7761 - val_loss: 0.5015 - val_binary_acc: 0.7630\n",
      "Epoch 26/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4857 - binary_acc: 0.7768 - val_loss: 0.5011 - val_binary_acc: 0.7656\n",
      "Epoch 27/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4847 - binary_acc: 0.7760 - val_loss: 0.4987 - val_binary_acc: 0.7648\n",
      "Epoch 28/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4848 - binary_acc: 0.7768 - val_loss: 0.5002 - val_binary_acc: 0.7647\n",
      "Epoch 29/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4841 - binary_acc: 0.7773 - val_loss: 0.5007 - val_binary_acc: 0.7660\n",
      "Epoch 30/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4850 - binary_acc: 0.7778 - val_loss: 0.4988 - val_binary_acc: 0.7664\n",
      "Epoch 31/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4862 - binary_acc: 0.7766 - val_loss: 0.5004 - val_binary_acc: 0.7671\n",
      "Epoch 32/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4849 - binary_acc: 0.7777 - val_loss: 0.5004 - val_binary_acc: 0.7665\n",
      "Epoch 33/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4845 - binary_acc: 0.7767 - val_loss: 0.4987 - val_binary_acc: 0.7684\n",
      "Epoch 34/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4841 - binary_acc: 0.7780 - val_loss: 0.4988 - val_binary_acc: 0.7688\n",
      "Epoch 35/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4840 - binary_acc: 0.7783 - val_loss: 0.5005 - val_binary_acc: 0.7689\n",
      "Epoch 36/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4838 - binary_acc: 0.7779 - val_loss: 0.5027 - val_binary_acc: 0.7662\n",
      "Epoch 37/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4840 - binary_acc: 0.7783 - val_loss: 0.5002 - val_binary_acc: 0.7677\n",
      "Epoch 38/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4850 - binary_acc: 0.7777 - val_loss: 0.5052 - val_binary_acc: 0.7695\n",
      "Epoch 39/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4853 - binary_acc: 0.7781 - val_loss: 0.5040 - val_binary_acc: 0.7687\n",
      "Epoch 40/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4850 - binary_acc: 0.7786 - val_loss: 0.5018 - val_binary_acc: 0.7699\n",
      "Epoch 41/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4849 - binary_acc: 0.7780 - val_loss: 0.5033 - val_binary_acc: 0.7684\n",
      "Epoch 42/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4856 - binary_acc: 0.7784 - val_loss: 0.5037 - val_binary_acc: 0.7678\n",
      "Epoch 43/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4854 - binary_acc: 0.7786 - val_loss: 0.5029 - val_binary_acc: 0.7706\n",
      "Epoch 44/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4851 - binary_acc: 0.7792 - val_loss: 0.5050 - val_binary_acc: 0.7671\n",
      "Epoch 45/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4849 - binary_acc: 0.7780 - val_loss: 0.5039 - val_binary_acc: 0.7698\n",
      "Epoch 46/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4855 - binary_acc: 0.7800 - val_loss: 0.5077 - val_binary_acc: 0.7655\n",
      "Epoch 47/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4854 - binary_acc: 0.7799 - val_loss: 0.5055 - val_binary_acc: 0.7672\n",
      "Epoch 48/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4859 - binary_acc: 0.7797 - val_loss: 0.5088 - val_binary_acc: 0.7707\n",
      "Epoch 49/50\n",
      "845/845 [==============================] - 3s 4ms/step - loss: 0.4856 - binary_acc: 0.7811 - val_loss: 0.5047 - val_binary_acc: 0.7703\n",
      "Epoch 50/50\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.4851 - binary_acc: 0.7797 - val_loss: 0.5073 - val_binary_acc: 0.7684\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "## make prediction\n",
    "pred_prob = model.predict([test_cont_feats, test_cate_feats]).flatten()\n",
    "pred_like = 1*(pred_prob >= 0.5)\n",
    "print(pred_like)\n",
    "_, binary_acc_test = model.evaluate(x=[test_cont_feats, test_cate_feats], y=test_like)\n",
    "print('mcr: SideNCF: %.3f' %binary_acc_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0 ... 1 0 1]\n",
      "1040/1040 [==============================] - 1s 992us/step - loss: 0.4977 - binary_acc: 0.7716\n",
      "mcr: SideNCF: 0.772\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ba986c4ce28ee590feb069d7dff47f6c0fdeef1e6e7e0640650ca3a5af9b036"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('shiing': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}