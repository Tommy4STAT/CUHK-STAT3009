{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook10(b): Top-K recommendation via AdaRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Part 1: report `NDCG` and `pairwise ranking 0-1 loss (PR_loss)` for existing methods\n",
    "\n",
    "Note that `NDCG` larger one is better; `PR_loss` smaller one is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a new evaluation metric ``NDCG`` and `PR_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import itertools\n",
    "\n",
    "def ndcg_rs(test_pair, true_rating, pred_rating, k=10):\n",
    "    ndcg = []\n",
    "    user_lst = list(set(test_pair[:,0]))\n",
    "    user_index = [np.where(test_pair[:,0] == user_tmp)[0] for user_tmp in user_lst]\n",
    "    for user_tmp in user_lst:\n",
    "        true_rating_tmp = true_rating[user_index[user_tmp]]\n",
    "        pred_rating_tmp = pred_rating[user_index[user_tmp]]\n",
    "        ndcg_tmp = ndcg_score([true_rating_tmp], [pred_rating_tmp], k=k)\n",
    "        ndcg.append(ndcg_tmp)\n",
    "    return np.mean(ndcg)\n",
    "\n",
    "def PR_loss(test_pair, true_rating, pred_rating):\n",
    "    PR_loss_lst = []\n",
    "    user_lst = list(set(test_pair[:,0]))\n",
    "    user_index = [np.where(test_pair[:,0] == user_tmp)[0] for user_tmp in user_lst]\n",
    "    for user_tmp in user_lst:\n",
    "        record_idx_tmp = user_index[user_tmp]\n",
    "        for pair_tmp in itertools.combinations(record_idx_tmp, 2):\n",
    "            diff_true = true_rating[pair_tmp[0]] - true_rating[pair_tmp[1]]\n",
    "            diff_pred = pred_rating[pair_tmp[0]] - pred_rating[pair_tmp[1]]\n",
    "            if diff_true != 0:\n",
    "            \tPR_loss_lst.append( 1*(diff_true*diff_pred <= 0) )\n",
    "    return np.mean(PR_loss_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load `MovieLen` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# load rating\n",
    "df = pd.read_csv('./dataset/ml-latest-small/ratings.csv')\n",
    "del df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping the user_id, movie_id to digits\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le_movie = preprocessing.LabelEncoder()\n",
    "le_user = preprocessing.LabelEncoder()\n",
    "\n",
    "df['movieId'] = le_movie.fit_transform(df['movieId'])\n",
    "df['userId'] = le_user.fit_transform(df['userId'])\n",
    "## generate train / test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dtest = train_test_split(df, test_size=0.33, random_state=42)\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tran_pair, train_rating\n",
    "train_pair = dtrain[['userId', 'movieId']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "# test_pair\n",
    "test_pair = dtest[['userId', 'movieId']].values\n",
    "n_user = max(train_pair[:,0].max(), test_pair[:,0].max())+1\n",
    "n_item = max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: report `NDCG` and `PR_loss` for existing RS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import SVG\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(NCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(16, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(8, name='fc-2', activation='relu')\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        concatted_vec = self.concatenate([user_vector, movie_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 20:16:46.199261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.205722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.206010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.206504: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-29 20:16:46.207240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.207491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.207716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.533792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.534004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.534152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-29 20:16:46.534298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 551 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = NCF(num_users=n_user, num_movies=n_item, embedding_size=20)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "    keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), \n",
    "    loss=keras.losses.MeanSquaredError(), \n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "845/845 [==============================] - 2s 1ms/step - loss: 1.3604 - mae: 0.7913 - rmse: 1.0451 - val_loss: 1.1589 - val_mae: 0.7298 - val_rmse: 0.9496\n",
      "Epoch 2/50\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1390 - mae: 0.7298 - rmse: 0.9357 - val_loss: 1.1494 - val_mae: 0.7325 - val_rmse: 0.9426\n",
      "Epoch 3/50\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1879 - mae: 0.7288 - rmse: 0.9355 - val_loss: 1.1028 - val_mae: 0.7379 - val_rmse: 0.9439\n",
      "Epoch 4/50\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1525 - mae: 0.7301 - rmse: 0.9360 - val_loss: 1.2050 - val_mae: 0.7449 - val_rmse: 0.9486\n",
      "Epoch 5/50\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1372 - mae: 0.7274 - rmse: 0.9330 - val_loss: 1.2148 - val_mae: 0.7370 - val_rmse: 0.9480\n",
      "Epoch 6/50\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1543 - mae: 0.7271 - rmse: 0.9331 - val_loss: 1.1553 - val_mae: 0.7333 - val_rmse: 0.9427\n",
      "Epoch 7/50\n",
      "801/845 [===========================>..] - ETA: 0s - loss: 1.1524 - mae: 0.7259 - rmse: 0.9306Restoring model weights from the end of the best epoch: 2.\n",
      "845/845 [==============================] - 1s 1ms/step - loss: 1.1541 - mae: 0.7263 - rmse: 0.9315 - val_loss: 1.2226 - val_mae: 0.7309 - val_rmse: 0.9494\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_rmse', min_delta=0, patience=5, verbose=1, \n",
    "    mode='auto', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_pair,\n",
    "    y=train_rating,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks = callbacks,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: NCF: 0.935\n",
      "NDCG: NCF: 0.878\n",
      "PR_loss: NCF: 0.387\n"
     ]
    }
   ],
   "source": [
    "## make prediction\n",
    "pred_rating = model.predict(test_pair).flatten()\n",
    "print('rmse: NCF: %.3f' %np.sqrt(np.mean((pred_rating - test_rating)**2)))\n",
    "print('NDCG: NCF: %.3f' %ndcg_rs(test_pair, true_rating=test_rating, pred_rating=pred_rating))\n",
    "print('PR_loss: NCF: %.3f' %PR_loss(test_pair, true_rating=test_rating, pred_rating=pred_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fit Neural networks with `PR_loss`\n",
    "\n",
    "## Step 1: pre-process the dataset\n",
    "- `pair`: (u, i) -> `triple`: (u, i, i')\n",
    "- `outcome`: np.sign( y_i - y_{i'} )\n",
    "\n",
    "## Step 2: Define model with two-level construction\n",
    "- `score` function, to produce the scores for each item under a user.\n",
    "- `diff` function, make different based on the `score`\n",
    "\n",
    "## Step 3: Fit the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: pre-processing data\n",
    "\n",
    "## get triple data\n",
    "def gen_triple(pair, rating):\n",
    "    triple, diff = [], []\n",
    "    ## user list\n",
    "    user_lst = list(set(pair[:,0]))\n",
    "    user_index = [np.where(pair[:,0] == user_tmp)[0] for user_tmp in user_lst]\n",
    "    for user_tmp in user_lst:\n",
    "        record_idx_tmp = user_index[user_tmp]\n",
    "        ## find all possible pairwise comparison of observed items under the users\n",
    "        for pair_idx_tmp in itertools.combinations(record_idx_tmp, 2):\n",
    "            diff_tmp = np.sign(rating[pair_idx_tmp[0]] - rating[pair_idx_tmp[1]])\n",
    "            ## if diff is zero; no information; remove this triple\n",
    "            if diff_tmp != 0:\n",
    "                triple.append([user_tmp, pair[pair_idx_tmp[0], 1], pair[pair_idx_tmp[1], 1]])\n",
    "                diff.append(diff_tmp)\n",
    "    return np.array(triple), np.array(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triple, train_diff = gen_triple(pair=train_pair, rating=train_rating)\n",
    "## change data (-1,1) to (0,1) type\n",
    "train_diff = (.5*(train_diff+1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define ranking model with `score` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class RankNCF(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RankNCF, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-2),\n",
    "        )\n",
    "        self.concatenate = layers.Concatenate()\n",
    "        self.dense1 = layers.Dense(16, name='fc-1', activation='relu')\n",
    "        self.dense2 = layers.Dense(8, name='fc-2', activation='relu')\n",
    "        self.dense3 = layers.Dense(1, name='fc-3', activation='linear')\n",
    "\n",
    "    def scorer(self, user_id, movie_id):\n",
    "        user_vector = self.user_embedding(user_id)\n",
    "        movie_vector = self.movie_embedding(movie_id)\n",
    "        concatted_vec = self.concatenate([user_vector, movie_vector])\n",
    "        fc_1 = self.dense1(concatted_vec)\n",
    "        fc_2 = self.dense2(fc_1)\n",
    "        fc_3 = self.dense3(fc_2)\n",
    "        return fc_3\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id = inputs[:, 0]\n",
    "        movie1_id = inputs[:, 1]\n",
    "        movie2_id = inputs[:, 2]\n",
    "        score1 = self.scorer(user_id, movie1_id)\n",
    "        score2 = self.scorer(user_id, movie2_id)\n",
    "        return score1 - score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankNCF(num_users=n_user, num_movies=n_item, embedding_size=20)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.BinaryAccuracy(name='binary_acc')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), \n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "135492/135492 [==============================] - 192s 1ms/step - loss: 0.8168 - binary_acc: 0.6206 - val_loss: 0.8407 - val_binary_acc: 0.5771\n",
      "Epoch 2/50\n",
      "135492/135492 [==============================] - 190s 1ms/step - loss: 0.8144 - binary_acc: 0.6212 - val_loss: 0.8386 - val_binary_acc: 0.5812\n",
      "Epoch 3/50\n",
      "135492/135492 [==============================] - 189s 1ms/step - loss: 0.8136 - binary_acc: 0.6210 - val_loss: 0.8190 - val_binary_acc: 0.5797\n",
      "Epoch 4/50\n",
      "135492/135492 [==============================] - 189s 1ms/step - loss: 0.8134 - binary_acc: 0.6209 - val_loss: 0.8192 - val_binary_acc: 0.5842\n",
      "Epoch 5/50\n",
      "135492/135492 [==============================] - 190s 1ms/step - loss: 0.8136 - binary_acc: 0.6212 - val_loss: 0.8098 - val_binary_acc: 0.5814\n",
      "Epoch 6/50\n",
      "135492/135492 [==============================] - 188s 1ms/step - loss: 0.8135 - binary_acc: 0.6211 - val_loss: 0.8827 - val_binary_acc: 0.5734\n",
      "Epoch 7/50\n",
      "135492/135492 [==============================] - 189s 1ms/step - loss: 0.8146 - binary_acc: 0.6209 - val_loss: 0.8178 - val_binary_acc: 0.5805\n",
      "Epoch 8/50\n",
      "135492/135492 [==============================] - 188s 1ms/step - loss: 0.8140 - binary_acc: 0.6210 - val_loss: 0.8820 - val_binary_acc: 0.5774\n",
      "Epoch 9/50\n",
      "135457/135492 [============================>.] - ETA: 0s - loss: 0.8147 - binary_acc: 0.6211Restoring model weights from the end of the best epoch: 4.\n",
      "135492/135492 [==============================] - 194s 1ms/step - loss: 0.8147 - binary_acc: 0.6211 - val_loss: 0.8868 - val_binary_acc: 0.5811\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping( \n",
    "    monitor='val_binary_acc', min_delta=0, patience=5, verbose=1, \n",
    "    mode='max', baseline=None, restore_best_weights=True)]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_triple,\n",
    "    y=train_diff,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks = callbacks,\n",
    "    validation_split=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: RankNCF: 0.869\n",
      "PR_loss: RankNCF: 0.399\n"
     ]
    }
   ],
   "source": [
    "pred_rating = model.scorer(user_id = test_pair[:,0], movie_id = test_pair[:,1])\n",
    "pred_rating = pred_rating.numpy().flatten()\n",
    "\n",
    "print('NDCG: RankNCF: %.3f' %ndcg_rs(test_pair, true_rating=test_rating, pred_rating=pred_rating))\n",
    "print('PR_loss: RankNCF: %.3f' %PR_loss(test_pair, true_rating=test_rating, pred_rating=pred_rating))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ba986c4ce28ee590feb069d7dff47f6c0fdeef1e6e7e0640650ca3a5af9b036"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
