{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('shiing': conda)"
  },
  "interpreter": {
   "hash": "414c3711fdd48b2f544ed65b2d7540fc36858fd70ee4f390e2b75c8d3b465c57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook4: ALS for Latent Factor Models II"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ALS for Latent factor model\n",
    "\n",
    "- **Parameter**: \n",
    "  - \\#Users: `n`\n",
    "  - \\#Items: `m`\n",
    "  - latent factors for users: `P` \n",
    "  - latent factors for items: `Q`\n",
    "  - tuning parameter: `lam`\n",
    "  - \\#Latent factors: `K`\n",
    "\n",
    "\n",
    "- **Method**:\n",
    "  - `fit`: input: `train_pair`, `train_rating`; output: fitted `P` and `Q`\n",
    "  - `predict`: input: `test_pair`; output: predicted ratings\n",
    "  - `rmse`: input: `test_pair`, `test_rating`; output: RMSE for the predicted ratings\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def rmse(true, pred):\n",
    "\treturn np.sqrt(np.mean((pred - true)**2))\n",
    "\n",
    "class LFM(object):\n",
    "\n",
    "    def __init__(self, n_user, n_item, lam=.001, K=10, iterNum=10, tol=1e-4, verbose=1):\n",
    "        self.P = np.random.randn(n_user, K)\n",
    "        self.Q = np.random.randn(n_item, K)\n",
    "        # self.index_item = []\n",
    "        # self.index_user = []\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.lam = lam\n",
    "        self.K = K\n",
    "        self.iterNum = iterNum\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, train_pair, train_rating):\n",
    "        diff, tol = 1., self.tol\n",
    "        n_user, n_item, n_obs = self.n_user, self.n_item, len(train_pair)\n",
    "        K, iterNum, lam = self.K, self.iterNum, self.lam\n",
    "        ## store user/item index set\n",
    "        self.index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "        self.index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "        print('Fitting Reg-LFM: K: %d, lam: %.5f' %(K, lam))\n",
    "        for i in range(iterNum):\n",
    "            ## item update\n",
    "            score_old = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            for item_id in range(n_item):\n",
    "                index_item_tmp = self.index_item[item_id]\n",
    "                if len(index_item_tmp) == 0:\n",
    "                    self.Q[item_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_item_tmp:\n",
    "                    ## double-check\n",
    "                    if item_id != train_pair[record_ind][1]:\n",
    "                        raise ValueError('the item_id is waring in updating Q!')\n",
    "                    user_id, rating_tmp = train_pair[record_ind][0], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.P[user_id,:], self.P[user_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.P[user_id,:]                    \n",
    "                self.Q[item_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            \n",
    "            for user_id in range(n_user):\n",
    "                index_user_tmp = self.index_user[user_id]\n",
    "                if len(index_user_tmp) == 0:\n",
    "                    self.P[user_id,:] = 0.\n",
    "                    continue\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for record_ind in index_user_tmp:\n",
    "                    ## double-check\n",
    "                    if user_id != train_pair[record_ind][0]:\n",
    "                        raise ValueError('the user_id is waring in updating P!')\n",
    "                    item_id, rating_tmp = train_pair[record_ind][1], train_rating[record_ind]\n",
    "                    sum_matrix = sum_matrix + np.outer(self.Q[item_id,:], self.Q[item_id,:])\n",
    "                    sum_pu = sum_pu + rating_tmp * self.Q[item_id,:]                    \n",
    "                self.P[user_id,:] = np.dot(np.linalg.inv(sum_matrix + lam*n_obs*np.identity(K)), sum_pu)\n",
    "            # compute the new rmse score\n",
    "            score_new = self.rmse(test_pair=train_pair, test_rating=train_rating)\n",
    "            diff = abs(score_new - score_old) / score_old\n",
    "            if self.verbose:\n",
    "                print(\"Reg-LFM: ite: %d; diff: %.3f RMSE: %.3f\" %(i, diff, score_new))\n",
    "            if(diff < tol):\n",
    "                break\n",
    "\n",
    "    def predict(self, test_pair):\n",
    "        # predict ratings for user-item pairs\n",
    "        pred_rating = [np.dot(self.P[line[0]], self.Q[line[1]]) for line in test_pair]\n",
    "        return np.array(pred_rating)\n",
    "    \n",
    "    def rmse(self, test_pair, test_rating):\n",
    "        # report the rmse for the fitted `LFM`\n",
    "        pred_rating = self.predict(test_pair=test_pair)\n",
    "        return np.sqrt( np.mean( (pred_rating - test_rating)**2) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and pro-processed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dtrain = pd.read_csv('./dataset/train.csv')\n",
    "dtest = pd.read_csv('./dataset/test.csv')\n",
    "## save real ratings for test set for evaluation.\n",
    "test_rating = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')\n",
    "\n",
    "## convert string to user_id and item_id -> [user_id, item_id, rating]\n",
    "# pre-process for training data\n",
    "train_pair = dtrain[['user_id', 'movie_id']].values\n",
    "train_rating = dtrain['rating'].values\n",
    "# pre-process for testing set\n",
    "test_pair = dtest[['user_id', 'movie_id']].values\n",
    "\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit and predict based on `LFM`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# fitting\n",
    "shiing = LFM(n_user, n_item, K=3, lam=.0001)\n",
    "shiing.fit(train_pair=train_pair, train_rating=train_rating)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting Reg-LFM: K: 3, lam: 0.00010\n",
      "Reg-LFM: ite: 0; diff: 0.221 RMSE: 3.260\n",
      "Reg-LFM: ite: 1; diff: 0.681 RMSE: 1.041\n",
      "Reg-LFM: ite: 2; diff: 0.077 RMSE: 0.960\n",
      "Reg-LFM: ite: 3; diff: 0.025 RMSE: 0.936\n",
      "Reg-LFM: ite: 4; diff: 0.011 RMSE: 0.925\n",
      "Reg-LFM: ite: 5; diff: 0.006 RMSE: 0.920\n",
      "Reg-LFM: ite: 6; diff: 0.004 RMSE: 0.917\n",
      "Reg-LFM: ite: 7; diff: 0.002 RMSE: 0.914\n",
      "Reg-LFM: ite: 8; diff: 0.001 RMSE: 0.913\n",
      "Reg-LFM: ite: 9; diff: 0.001 RMSE: 0.913\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# pediction\n",
    "pred_rating = shiing.predict(test_pair)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# evaluation\n",
    "# rmse(test_rating, pred_rating)\n",
    "# or we can just use\n",
    "shiing.rmse(test_pair, test_rating)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.1813987511229975"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# baseline methods\n",
    "class glb_mean(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.glb_mean = 0\n",
    "\t\n",
    "\tdef fit(self, train_ratings):\n",
    "\t\tself.glb_mean = np.mean(train_ratings)\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))\n",
    "\t\tpred = pred*self.glb_mean\n",
    "\t\treturn pred\n",
    "\n",
    "class user_mean(object):\n",
    "\tdef __init__(self, n_user):\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.user_mean = np.zeros(n_user)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_ratings):\n",
    "\t\tself.glb_mean = train_ratings.mean()\n",
    "\t\tfor u in range(self.n_user):\n",
    "\t\t\tind_train = np.where(train_pair[:,0] == u)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.user_mean[u] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.user_mean[u] = train_ratings[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.user_mean[user_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred\n",
    "\n",
    "class item_mean(object):\n",
    "\tdef __init__(self, n_item):\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.glb_mean = 0.\n",
    "\t\tself.item_mean = np.zeros(n_item)\n",
    "\t\n",
    "\tdef fit(self, train_pair, train_ratings):\n",
    "\t\tself.glb_mean = train_ratings.mean()\n",
    "\t\tfor i in range(self.n_item):\n",
    "\t\t\tind_train = np.where(train_pair[:,1] == i)[0]\n",
    "\t\t\tif len(ind_train) == 0:\n",
    "\t\t\t\tself.item_mean[i] = self.glb_mean\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.item_mean[i] = train_ratings[ind_train].mean()\n",
    "\t\n",
    "\tdef predict(self, test_pair):\n",
    "\t\tpred = np.ones(len(test_pair))*self.glb_mean\n",
    "\t\tj = 0\n",
    "\t\tfor row in test_pair:\n",
    "\t\t\tuser_tmp, item_tmp = row[0], row[1]\n",
    "\t\t\tpred[j] = self.item_mean[item_tmp]\n",
    "\t\t\tj = j + 1\n",
    "\t\treturn pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## Baseline + LFM\n",
    "# glb mean\n",
    "glb_ave = glb_mean()\n",
    "glb_ave.fit(train_rating)\n",
    "pred = glb_ave.predict(test_pair)\n",
    "# user_mean\n",
    "train_rating_cm = train_rating - glb_ave.predict(train_pair)\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_ratings=train_rating_cm)\n",
    "train_rating_res = train_rating_cm - user_ave.predict(train_pair)\n",
    "pred = pred + user_ave.predict(test_pair)\n",
    "# fit correlation-based RS by residual ratings \n",
    "shiing = LFM(n_user, n_item, K=3, lam=.0001)\n",
    "shiing.fit(train_pair=train_pair, train_rating=train_rating_res)\n",
    "pred = pred + shiing.predict(test_pair)\n",
    "\n",
    "print('RMSE for glb + user_mean + LFM: %.3f' %rmse(test_rating, pred) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting Reg-LFM: K: 3, lam: 0.00010\n",
      "Reg-LFM: ite: 0; diff: 0.519 RMSE: 0.944\n",
      "Reg-LFM: ite: 1; diff: 0.044 RMSE: 0.902\n",
      "Reg-LFM: ite: 2; diff: 0.031 RMSE: 0.875\n",
      "Reg-LFM: ite: 3; diff: 0.024 RMSE: 0.854\n",
      "Reg-LFM: ite: 4; diff: 0.016 RMSE: 0.840\n",
      "Reg-LFM: ite: 5; diff: 0.010 RMSE: 0.832\n",
      "Reg-LFM: ite: 6; diff: 0.006 RMSE: 0.827\n",
      "Reg-LFM: ite: 7; diff: 0.004 RMSE: 0.823\n",
      "Reg-LFM: ite: 8; diff: 0.003 RMSE: 0.821\n",
      "Reg-LFM: ite: 9; diff: 0.002 RMSE: 0.819\n",
      "RMSE for glb + user_mean + LFM: 0.975\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Which `lam` and `K` is best for prediction?\n",
    "\n",
    "- Grid search based on cross-validation\n",
    "- models with different `lam` and `K`: print CV rmse score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LFM_CV(object):\n",
    "\n",
    "\tdef __init__(self, n_user, n_item, cv=5,\n",
    "\t\t\t\tlams=[.000001,.0001,.001,.01], \n",
    "\t\t\t\tKs=[3,5,10,20], \n",
    "\t\t\t\titerNum=10, tol=1e-4):\n",
    "\t\t# self.index_item = []\n",
    "\t\t# self.index_user = []\n",
    "\t\tself.n_user = n_user\n",
    "\t\tself.n_item = n_item\n",
    "\t\tself.cv = 5\n",
    "\t\tself.lams = lams\n",
    "\t\tself.Ks = Ks\n",
    "\t\tself.iterNum = iterNum\n",
    "\t\tself.tol = tol\n",
    "\t\tself.best_model = {}\n",
    "\t\tself.cv_result = {'K': [], 'lam': [], 'train_rmse': [], 'valid_rmse': []}\n",
    "\n",
    "\tdef grid_search(self, train_pair, train_rating):\n",
    "\t\t## generate all comb of `K` and `lam`\n",
    "\t\tkf = KFold(n_splits=self.cv, shuffle=True)\n",
    "\t\tfor (K,lam) in itertools.product(self.Ks, self.lams):\n",
    "\t\t\ttrain_rmse_tmp, valid_rmse_tmp = 0., 0.\n",
    "\t\t\tfor train_index, valid_index in kf.split(train_pair):\n",
    "\t\t\t\t# produce training/validation sets\n",
    "\t\t\t\ttrain_pair_cv, train_rating_cv = train_pair[train_index], train_rating[train_index]\n",
    "\t\t\t\tvalid_pair_cv, valid_rating_cv = train_pair[valid_index], test_rating[valid_index]\n",
    "\t\t\t\t# fit the model based on CV data\n",
    "\t\t\t\tmodel_tmp = LFM(self.n_user, self.n_item, K=K, lam=lam, verbose=0)\n",
    "\t\t\t\tmodel_tmp.fit(train_pair=train_pair_cv, train_rating=train_rating_cv)\n",
    "\t\t\t\ttrain_rmse_tmp += model_tmp.rmse(test_pair=train_pair_cv, test_rating=train_rating_cv) / self.cv\n",
    "\t\t\t\tvalid_rmse_tmp += model_tmp.rmse(test_pair=valid_pair_cv, test_rating=valid_rating_cv) / self.cv\n",
    "\t\t\tself.cv_result['K'].append(K)\n",
    "\t\t\tself.cv_result['lam'].append(lam)\n",
    "\t\t\tself.cv_result['train_rmse'].append(train_rmse_tmp)\n",
    "\t\t\tself.cv_result['valid_rmse'].append(valid_rmse_tmp)\n",
    "\t\tself.cv_result = pd.DataFrame.from_dict(self.cv_result)\n",
    "\t\tbest_ind = self.cv_result['valid_rmse'].argmin()\n",
    "\t\tself.best_result = self.cv_result.loc[best_ind]\n",
    "\t\n",
    "\tdef plot_grid(self, data_source='valid'):\n",
    "\t\tsns.set_theme()\n",
    "\t\tif data_source == 'train':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"train_rmse\")\n",
    "\t\telif data_source == 'valid':\n",
    "\t\t\tcv_pivot = self.cv_result.pivot(\"K\", \"lam\", \"valid_rmse\")\n",
    "\t\telse:\n",
    "\t\t\traise ValueError('data_source must be train or valid!')\n",
    "\t\tsns.heatmap(cv_pivot, annot=True, fmt=\".3f\", linewidths=.5, cmap=\"YlGnBu\")\n",
    "\t\tplt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## CV based on `LFM_CV`\n",
    "## Baseline + LFM\n",
    "glb_ave = glb_mean()\n",
    "glb_ave.fit(train_rating)\n",
    "pred = glb_ave.predict(test_pair)\n",
    "# user_mean\n",
    "train_rating_cm = train_rating - glb_ave.predict(train_pair)\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_ratings=train_rating_cm)\n",
    "train_rating_res = train_rating_cm - user_ave.predict(train_pair)\n",
    "pred = pred + user_ave.predict(test_pair)\n",
    "# fit LFM_CV by residual ratings \n",
    "Ks, lams = [2, 3, 5, 10], 10**np.arange(-6, -2, .5)\n",
    "shiing_cv = LFM_CV(n_user, n_item, cv=5, Ks=Ks, lams=lams)\n",
    "shiing_cv.grid_search(train_pair, train_rating_res)\n",
    "shiing_cv.plot_grid('valid')\n",
    "shiing_cv.plot_grid('train')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## refit the best model, and make prediction\n",
    "pred = pred + shiing.predict(test_pair)\n",
    "shiing_cv = LFM_CV(n_user, n_item, cv=3, Ks=Ks, lams=lams)\n",
    "\n",
    "print('RMSE for glb + user_mean + LFM: %.3f' %rmse(test_rating, pred))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}