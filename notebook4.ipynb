{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('shiing': conda)"
  },
  "interpreter": {
   "hash": "414c3711fdd48b2f544ed65b2d7540fc36858fd70ee4f390e2b75c8d3b465c57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook4: ALS for Latent Factor Models II"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ALS for Latent factor model\n",
    "\n",
    "- Parameter: \n",
    "  - \\#Users: `n`\n",
    "  - \\#Items: `m`\n",
    "  - latent factors for users: `P` \n",
    "  - latent factors for items: `Q`\n",
    "  - tuning parameter: `lam`\n",
    "  - \\#Latent factors: `K`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class LFM(object):\n",
    "\n",
    "    def __init__(self, n_user, n_item, lam=1., K=5, iterNum=10, tol=1e-4):\n",
    "        self.P = np.random.randn((n_user, K))\n",
    "        self.Q = np.random.randn((n_item, K))\n",
    "        self.index_item = []\n",
    "        self.index_user = []\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.lam = lam\n",
    "        self.K = K\n",
    "        self.iterNum = iterNum\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, train_pair, train_rating):\n",
    "        diff, tol = 1., self.tol\n",
    "        n_user, n_item, n_obs = self.n_user, self.n_item, len(train_pair)\n",
    "        K, iterNum, lam = self.K, self.iterNum, self.lam\n",
    "        ## store user/item index set\n",
    "        self.index_item = [np.where(train_pair[:,1] == i)[0] for i in range(n_item)]\n",
    "        self.index_user = [np.where(train_pair[:,0] == u)[0] for u in range(n_user)]\n",
    "        \n",
    "        Q_old, P_old = self.P.copy(), self.Q.copy()\n",
    "        # Q_res, P_res = np.ones((m,K)), np.ones((n,K))\n",
    "        for i in range(iterNum):\n",
    "            ## item update\n",
    "            for item_id in range(n_item):\n",
    "                index_item_tmp = self.index_item[item_id]\n",
    "                if len(user_list_tmp) == 0:\n",
    "                    continue\n",
    "                user_list_tmp = train_pair[index_item_tmp][:,0]\n",
    "                sum_pu, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                for user_id in user_list_tmp:\n",
    "                    sum_matrix = sum_matrix + np.outer(self.P[user_id,:], self.P[user_id,:])\n",
    "                    sum_pu = sum_pu + train_data[train_index_matrix[user_id+1, item_id+1]-1, 2] * P_old[user_id,:]\n",
    "                Q[item_id,:] = np.dot(np.linalg.inv(sum_matrix + lam * np.identity(K)), sum_pu)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            diff1 = np.linalg.norm(Q - Q_old) / K / m\n",
    "            Q_res = Q_old\n",
    "            Q_old = np.array(Q)\n",
    "            with pymp.Parallel(self.n_jobs) as q:\n",
    "                ## iteration for users\n",
    "                for user_id in q.range(n):\n",
    "                    item_list_tmp = train_data[train_data[:,0] == user_id+1, 1]-1\n",
    "                    if len(item_list_tmp) == 0:\n",
    "                        continue\n",
    "                    sum_qi, sum_matrix = np.zeros((K)), np.zeros((K, K))\n",
    "                    for item_id in item_list_tmp:\n",
    "                        sum_matrix = sum_matrix + np.outer(Q_old[item_id,:], Q_old[item_id,:])\n",
    "                        sum_qi = sum_qi + train_data[train_index_matrix[user_id+1, item_id+1]-1, 2] * Q_old[item_id,:]\n",
    "                    P[user_id,:] = np.dot(np.linalg.inv(sum_matrix + lam * np.identity(K)), sum_qi)\n",
    "                    if user_id % (n/50) == 0:\n",
    "                        sys.stdout.write('.')\n",
    "                        sys.stdout.flush()\n",
    "            diff2 = np.linalg.norm(P - P_old) / K / n\n",
    "            # print \"\\n users update finished.\"\n",
    "            P_res = P_old\n",
    "            P_old = np.array(P)\n",
    "            diff = diff1 + diff2\n",
    "            # diff = np.linalg.norm(np.dot(P,Q.T) - np.dot(P_res,Q_res.T)) / n / m\n",
    "            SE = np.array([(line[2] - np.dot(P[int(line[0])-1], Q[int(line[1])-1]))**2 for line in train_data])\n",
    "            RMSE = np.sqrt(SE.mean())\n",
    "            print(\"\\n Regularized-SVD - K: %s lam: %s diff: %s RMSE: %s\" %(K, lam, diff, RMSE))\n",
    "            if(diff < tol):\n",
    "                break\n",
    "        self.Q, self.P = Q, P"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and pro-processed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dtrain = pd.read_csv('./dataset/train.csv')\n",
    "dtest = pd.read_csv('./dataset/test.csv')\n",
    "## save real ratings for test set for evaluation.\n",
    "test_ratings = np.array(dtest['rating'])\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dtest = dtest.drop(columns='rating')\n",
    "\n",
    "## convert string to user_id and item_id -> [user_id, item_id, rating]\n",
    "# pre-process for training data\n",
    "train_pair = dtrain[['user_id', 'movie_id']].values\n",
    "train_ratings = dtrain['rating'].values\n",
    "# pre-process for testing set\n",
    "test_pair = dtest[['user_id', 'movie_id']].values\n",
    "\n",
    "n_user, n_item = max(train_pair[:,0].max(), test_pair[:,0].max())+1, max(train_pair[:,1].max(), test_pair[:,1].max())+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define and training the predictive models based on `class`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## baseline user mean methods\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_ratings=train_ratings)\n",
    "pred_user = user_ave.predict(test_pair)\n",
    "print('RMSE for user_mean: %.3f' %rmse(test_ratings, pred_user) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for user_mean: 1.017\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "## baseline item mean methods\n",
    "item_ave = item_mean(n_item=n_item)\n",
    "item_ave.fit(train_pair=train_pair, train_ratings=train_ratings)\n",
    "pred_item = item_ave.predict(test_pair)\n",
    "print('RMSE for item_mean: %.3f' %rmse(test_ratings, pred_item) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for item_mean: 1.052\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "## Correlation-based RS (user)\n",
    "cor_user = cor_rs_user(n_user=n_user)\n",
    "cor_user.fit(train_pair=train_pair, train_ratings=train_ratings)\n",
    "pred_cor_user = cor_user.predict(test_pair, train_ratings)\n",
    "print('RMSE for item_mean: %.3f' %rmse(test_ratings, pred_cor_user) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for item_mean: 1.073\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "## Correlation-based RS (item)\n",
    "cor_item = cor_rs_item(n_item=n_item)\n",
    "cor_item.fit(train_pair=train_pair, train_ratings=train_ratings)\n",
    "pred_cor_item = cor_item.predict(test_pair, train_ratings)\n",
    "print('RMSE for item_mean: %.3f' %rmse(test_ratings, pred_cor_item) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for item_mean: 1.074\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## Baseline + Correlation-based RS\n",
    "# glb mean\n",
    "glb_ave = glb_mean()\n",
    "glb_ave.fit(train_ratings)\n",
    "pred = glb_ave.predict(test_pair)\n",
    "# user_mean\n",
    "train_ratings_cm = train_ratings - glb_ave.predict(train_pair)\n",
    "user_ave = user_mean(n_user=n_user)\n",
    "user_ave.fit(train_pair=train_pair, train_ratings=train_ratings_cm)\n",
    "train_ratings_res = train_ratings_cm - user_ave.predict(train_pair)\n",
    "pred = pred + user_ave.predict(test_pair)\n",
    "# fit correlation-based RS by residual ratings \n",
    "cor_user = cor_rs_user(n_user=n_user)\n",
    "cor_user.fit(train_pair=train_pair, train_ratings=train_ratings_res)\n",
    "pred = pred + cor_user.predict(test_pair, train_ratings_res, top=1000)\n",
    "\n",
    "print('RMSE for glb + user_mean + cor_rs(user): %.3f' %rmse(test_ratings, pred) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for ite\tm_mean: 1.005\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}