{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('shiing': conda)"
  },
  "interpreter": {
   "hash": "414c3711fdd48b2f544ed65b2d7540fc36858fd70ee4f390e2b75c8d3b465c57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CUHK [STAT3009](https://www.bendai.org/STAT3009/) Notebook1: Baseline methods for Recommender Systems"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Software prepare\n",
    "- `Code Editor`: VS Code; Sublime; or Atom\n",
    "\n",
    "- `Terminal`: Iterm2 in Mac; Deepin terminal in Linux"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating virtual environments\n",
    "- If you have multiple versions of Python on your system, you can select a specific Python version by running python3 or whichever version you want.\n",
    "\n",
    "- To create a virtual environment, decide upon a directory where you want to place it, and run the venv module as a script with the directory path:\n",
    "\n",
    "- How to create and activate a virtual environment, see Section 12.2 in the [Document]((https://docs.python.org/3/tutorial/venv.html)).\n",
    "\n",
    "- Install packages via `pip`, see `Installing packages` section in the [Document]((https://docs.python.org/3/tutorial/venv.html)).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Kaggle competition about recommender systems: user and item can be extended to more general cases.\n",
    "- [Elo Merchant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation/data?select=Data+Dictionary.xlsx): `merchant_id` and `card_id`.\n",
    "\n",
    "- [WSDM - KKBox's Music Recommendation Challenge](https://www.kaggle.com/c/kkbox-music-recommendation-challenge/data): `user` and `music`.\n",
    "\n",
    "- [Event Recommendation Engine Challenge](https://www.kaggle.com/c/event-recommendation-engine-challenge/overview/evaluation): `user` and `event`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load dataset into Python\n",
    "- Dowload [Netflix Prize Data](https://www.kaggle.com/netflix-inc/netflix-prize-data).\n",
    "\n",
    "- Load data into Python.\n",
    "\n",
    "- Re-orginize the data structure as a standard form.\n",
    "\n",
    "- For testing set, we hide the real ratings.\n",
    "\n",
    "- We only take the first subset for illustration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0 (user_id, item_id)   ratings\n",
       "14          1071         (867, 419)  6.321420\n",
       "1970        2989         (644, 128)  1.904344\n",
       "2083        2178          (69, 361) -3.877432\n",
       "1996        2818          (491, 75)  2.476299\n",
       "245         3193          (445, 95)  3.699980"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>(user_id, item_id)</th>\n      <th>ratings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14</th>\n      <td>1071</td>\n      <td>(867, 419)</td>\n      <td>6.321420</td>\n    </tr>\n    <tr>\n      <th>1970</th>\n      <td>2989</td>\n      <td>(644, 128)</td>\n      <td>1.904344</td>\n    </tr>\n    <tr>\n      <th>2083</th>\n      <td>2178</td>\n      <td>(69, 361)</td>\n      <td>-3.877432</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>2818</td>\n      <td>(491, 75)</td>\n      <td>2.476299</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>3193</td>\n      <td>(445, 95)</td>\n      <td>3.699980</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/train_demo.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Unnamed: 0 (user_id, item_id)\n",
       "163        1788         (276, 406)\n",
       "394        1379         (199, 292)\n",
       "281        2344         (781, 213)\n",
       "146         865          (455, 69)\n",
       "183        2261         (493, 258)"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>(user_id, item_id)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>163</th>\n      <td>1788</td>\n      <td>(276, 406)</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>1379</td>\n      <td>(199, 292)</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>2344</td>\n      <td>(781, 213)</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>865</td>\n      <td>(455, 69)</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>2261</td>\n      <td>(493, 258)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dt = pd.read_csv('./dataset/test_demo.csv')\n",
    "## save real ratings for test set for evaluation.\n",
    "test_ratings = dt['ratings']\n",
    "## remove the ratings in the test set to simulate prediction\n",
    "dt = dt.drop(columns='ratings')\n",
    "dt.sample(5)"
   ]
  },
  {
   "source": [
    "## Pre-process the data as a standard form\n",
    "- Convert `string` '(user_id, item_id)' -> `np.array` int \\[user_id, item_id\\]\n",
    "\n",
    "- Tutorial: [Reading Data from the Web: Web Scraping & Regular Expressions](https://www.summet.com/dmsi/html/readingTheWeb.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert string to user_id and item_id -> [user_id, item_id, rating]\n",
    "import re\n",
    "# pre-process for training data\n",
    "train_pair = [re.findall(r'\\d+', tmp) for tmp in df['(user_id, item_id)']]\n",
    "train_pair = np.array(train_pair)\n",
    "train_pair = train_pair.astype(int)\n",
    "# pre-process for testing set\n",
    "test_pair = [re.findall(r'\\d+', tmp) for tmp in dt['(user_id, item_id)']]\n",
    "test_pair = np.array(test_pair)\n",
    "test_pair = test_pair.astype(int)\n",
    "n_user, n_item = train_pair[:,0].max()+1, train_pair[:,1].max()+1\n",
    "train_ratings = df['ratings'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the pre-process data use `np.save`\n",
    "np.save('./dataset/train_pair.npy', train_pair)\n",
    "np.save('./dataset/test_pair.npy', test_pair)\n",
    "np.save('./dataset/train_ratings.npy', ratings)\n",
    "np.save('./dataset/test_ratings.npy', test_ratings)"
   ]
  },
  {
   "source": [
    "## Implement Baseline methods: global\\_average, user\\_average and item\\_average (For your practice)\n",
    "- Inpout: training set.\n",
    "\n",
    "- Output: return predicted ratings for (user id, item id) user-item pairs in testing set.\n",
    "\n",
    "- Goal: make prediction for testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(len(test_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.16038824 0.16038824 0.16038824 0.16038824 0.16038824 0.16038824\n 0.16038824 0.16038824 0.16038824 0.16038824]\n"
     ]
    }
   ],
   "source": [
    "## Global average\n",
    "global_pred = pred.copy()\n",
    "global_mean = ratings.mean()\n",
    "global_pred = global_mean*np.ones(len(pred))\n",
    "print(global_pred[:10])"
   ]
  },
  {
   "source": [
    "### user\\_average\n",
    "- Loop for all users\n",
    "    - Find all records for this user in both training and testing sets.\n",
    "    - Compute the average ratings for this user in the training set.\n",
    "    - Predict the ratings for this users in the testing set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.16038824 0.16038824 0.16038824 0.16038824 0.16038824 0.16038824\n 0.16038824 0.16038824 0.16038824 0.16038824]\n"
     ]
    }
   ],
   "source": [
    "## user average\n",
    "UA_pred = pred.copy()\n",
    "for u in range(n_user):\n",
    "    # find the index for both train and test for user_id = u\n",
    "    ind_test = np.where(test_pair[:,0] == u)[0]\n",
    "    ind_train = np.where(train_pair[:,0] == u)[0]\n",
    "    if len(ind_test) == 0:\n",
    "        continue\n",
    "    if len(ind_train) < 3:\n",
    "        UA_pred[ind_test] = global_mean\n",
    "    else:\n",
    "        # predict as user average\n",
    "        UA_pred[ind_test] = ratings[ind_train].mean()\n",
    "print(UA_pred[:10])"
   ]
  },
  {
   "source": [
    "## Evaluation: compute RMSE for baseline methods\n",
    "- Input: (1) predicted testing ratings (2) real testing ratings\n",
    "\n",
    "- Output: RMSE for the prediction\n",
    "\n",
    "- Goal: evaluate the prediction performance for the method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for GLB average: 2.588\n"
     ]
    }
   ],
   "source": [
    "## RMSE for Global average\n",
    "rmse_glb = np.sqrt(np.mean((global_pred - test_ratings)**2))\n",
    "print('RMSE for GLB average: %.3f' %rmse_glb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE for user average: 2.591\n"
     ]
    }
   ],
   "source": [
    "## RMSE for user average\n",
    "rmse_usr = np.sqrt(np.mean((UA_pred - test_ratings)**2))\n",
    "print('RMSE for user average: %.3f' %rmse_usr)"
   ]
  },
  {
   "source": [
    "## Summarize `glb_average` and `user_average` methods as Python functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### `glb_average`\n",
    "\n",
    "- *Input*: 'train_ratings', 'test_pair'\n",
    "\n",
    "- *Return*: Predicted ratings based on glb mean."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glb_mean(train_ratings, test_pair):\n",
    "    pred = train_ratings.mean() * np.ones(len(test_pair))\n",
    "    return pred"
   ]
  },
  {
   "source": [
    "### `user_average`\n",
    "\n",
    "- *Input*: 'train_pair', 'train_ratings', 'test_pair'\n",
    "\n",
    "- *Return*: Predicted ratings based on user mean."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_mean(train_pair, train_ratings, test_pair):\n",
    "    n_user = train_pair[:,0].max()+1\n",
    "    pred = np.zeros(len(test_pair))\n",
    "    glb_mean_value = train_ratings.mean()\n",
    "    for u in range(n_user):\n",
    "        # find the index for both train and test for user_id = u\n",
    "        ind_test = np.where(test_pair[:,0] == u)[0]\n",
    "        ind_train = np.where(train_pair[:,0] == u)[0]\n",
    "        if len(ind_test) == 0:\n",
    "            continue\n",
    "        if len(ind_train) < 3:\n",
    "            pred[ind_test] = glb_mean_value\n",
    "        else:\n",
    "            # predict as user average\n",
    "            pred[ind_test] = ratings[ind_train].mean()\n",
    "    return pred"
   ]
  },
  {
   "source": [
    "## Summarize `Evaluation` as a Python function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    return np.sqrt(np.mean((pred - true)**2))"
   ]
  }
 ]
}